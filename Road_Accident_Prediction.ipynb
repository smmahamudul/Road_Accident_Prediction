{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "Each year, 1.35 million p.eople are killed on roadways around the world. Crash injuries are estimated to be the eighth leading cause of death globally for all age groups and the leading cause of death for children and young people 5–29 years of age. More people now die in crashes than from HIV/AIDS.\n",
        "\n",
        "Accidents are an unfortunate reality of our daily lives, impacting individuals, families, and communities across the United States. Understanding the causes, patterns, and consequences of accidents is of paramount importance for public safety, transportation planning, and policy-making. In this notebook, we delve into the world of accidents, specifically focusing on accidents that occur on the roads and highways of the United States.\n",
        "\n",
        "**US Accidents Dataset**: Accurate and comprehensive data on accidents is essential for making informed decisions and improving safety measures. Fortunately, we have access to a rich and extensive dataset known as the \"US Accidents\" dataset. This dataset provides a wealth of information about traffic accidents that have occurred in various parts of the United States.\n",
        "\n",
        "In this notebook, we will embark on a data exploration journey to gain insights into US accidents. We will employ data analysis and visualization techniques to uncover patterns, trends, and important factors associated with accidents. We will also seek to answer crucial questions such as :\n",
        "\n",
        "\n",
        "By addressing these questions and more, we aim to contribute to a better understanding of accidents in the US. Our analysis may provide valuable insights for various stakeholders, including government agencies, law enforcement, transportation authorities, and the general public, in their efforts to enhance road safety and reduce the frequency and severity of accidents.\n",
        "\n",
        "Whether you are a data scientist, a traffic safety advocate, or simply someone interested in knowing more about the dynamics of accidents in the United States, this notebook is designed to inform and inspire. So, let's embark on this data-driven journey and explore the fascinating world of US accidents together."
      ],
      "metadata": {
        "id": "pGPGcqCQuQDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why did we choose this topic in particular?**\n",
        "analyzing accidents and predicting the severity of accidents is of paramount importance for various reasons, including public safety, transportation efficiency, and business solutions. Let's delve deeper into these aspects:\n",
        "\n",
        "**1. Public Safety and Human Lives:** Accidents on the road can have devastating consequences, resulting in injuries, fatalities, and long-lasting trauma for individuals and families. Analyzing accidents helps us identify the root causes and risk factors, allowing us to implement preventive measures and reduce the human toll of accidents. Prioritizing safety on our roads is a fundamental moral imperative.\n",
        "\n",
        "**2. Transportation Efficiency:** Accidents can cause traffic congestion, delays, and economic losses due to increased travel times and vehicle damage. Analyzing accident data enables transportation authorities to identify accident-prone areas, optimize traffic flow, and make infrastructure improvements. Reducing accidents can lead to smoother traffic, shorter commutes, and lower transportation costs.\n",
        "\n",
        "**3. Business Solutions and Cost Reduction:** For businesses that rely on transportation, such as logistics and delivery companies, accidents can result in disruptions, increased expenses, and damage to their reputation. Analyzing accident data helps these businesses identify high-risk routes and times, allowing them to optimize their operations, reduce accidents, and lower associated costs.\n",
        "\n",
        "**4. Resource Allocation:** Law enforcement agencies and emergency services often have limited resources. Predicting the severity of accidents can help allocate resources more effectively. By prioritizing severe accidents, first responders can arrive at the scene faster, potentially saving lives and reducing the severity of injuries.\n",
        "\n",
        "**5. Infrastructure Planning:** City planners and engineers can use accident data to inform the design and maintenance of road infrastructure. Understanding where accidents frequently occur and their causes can lead to safer road designs and targeted improvements, such as better signage, lighting, or road surface maintenance.\n",
        "\n",
        "**6. Insurance and Risk Assessment:** Insurance companies can use accident data to assess risk and set premiums. Predicting the severity of accidents helps insurers better understand the potential costs associated with policies, allowing them to offer competitive rates while ensuring they can cover claims.\n",
        "\n",
        "**7. Autonomous Vehicles and Advanced Driver Assistance Systems (ADAS):** The development of self-driving cars and ADAS relies heavily on accident data. Analyzing accidents helps improve the safety features and decision-making algorithms of autonomous vehicles, potentially reducing accidents caused by human error.\n",
        "\n",
        "**8. Policy and Legislation:** Government agencies and policymakers can use accident data to formulate evidence-based policies and regulations aimed at improving road safety. This may include stricter enforcement of traffic laws, enhanced driver education programs, or initiatives to reduce distracted driving.\n",
        "\n",
        "**Predicting Severity:** Predicting the severity of accidents is particularly important because it allows for timely and appropriate responses. By assessing accident severity, responders can allocate resources, prioritize medical treatment, and dispatch appropriate personnel. Predictive models can take into account various factors such as road conditions, weather, vehicle type, and collision type to estimate the likelihood of severe outcomes, helping improve emergency response and medical care.\n",
        "\n",
        "In conclusion, analyzing accidents and predicting their severity is not only a matter of public safety but also a multifaceted approach with significant implications for businesses, transportation, healthcare, and society as a whole. It empowers stakeholders to make informed decisions, allocate resources efficiently, and work collectively towards the goal of reducing accidents and their impact on people's lives."
      ],
      "metadata": {
        "id": "dkLNUBqfuQED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **About the Dataset**"
      ],
      "metadata": {
        "id": "nMKySC3TuQEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a countrywide car accident dataset that covers 49 states of the USA. The accident data were collected from February 2016 to March 2023, using multiple APIs that provide streaming traffic incident (or event) data. These APIs broadcast traffic data captured by various entities, including the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road networks. The dataset currently contains approximately 7.7 million accident records. For more information about this dataset, please visit here.\n",
        "\n",
        "\n",
        "This dataset was collected in real-time using multiple Traffic APIs. It contains accident data collected from February 2016 to March 2023 for the Contiguous United States.\n",
        "\n",
        "**Description for Each and Every Columns**:\n",
        "*This Data comprises of 46 columns and 7,728,394 rows*\n",
        "\n",
        "**ID**: This is a unique identifier of the accident record.\n",
        "\n",
        "**Source**: Source of raw accident data\n",
        "\n",
        "**Severity**: Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay).\n",
        "\n",
        "**Start_Time**: Shows start time of the accident in local time zone.\n",
        "\n",
        "**End_Time**: Shows end time of the accident in local time zone. End time here refers to when the impact of accident on traffic flow was dismissed.\n",
        "\n",
        "**Start_Lat**: Shows latitude in GPS coordinate of the start point.\n",
        "\n",
        "**Start_Lng**: Shows longitude in GPS coordinate of the start point.\n",
        "\n",
        "**End_Lat**: Shows latitude in GPS coordinate of the end point.\n",
        "\n",
        "**End_Lng**: Shows longitude in GPS coordinate of the end point.\n",
        "\n",
        "**Distance(mi)**: The length of the road extent affected by the accident in miles.\n",
        "\n",
        "**Description**: Shows a human provided description of the accident.\n",
        "\n",
        "**Street**: Shows the street name in address field.\n",
        "\n",
        "**City**: Shows the city in address field.\n",
        "\n",
        "**County**: Shows the county in address field.\n",
        "\n",
        "**State**: Shows the state in address field.\n",
        "\n",
        "**Zipcode**: Shows the zipcode in address field.\n",
        "\n",
        "**Country**: Shows the country in address field.\n",
        "\n",
        "**Timezone**: Shows timezone based on the location of the accident (eastern, central, etc.).\n",
        "\n",
        "**Airport_Code**: Denotes an airport-based weather station which is the closest one to location of the accident.\n",
        "\n",
        "**Weather_Timestamp**: Shows the time-stamp of weather observation record (in local time).\n",
        "\n",
        "**Temperature(F)**: Shows the temperature (in Fahrenheit).\n",
        "\n",
        "**Wind_Chill(F):** Shows the wind chill (in Fahrenheit).\n",
        "\n",
        "**Humidity(%)**: Shows the humidity (in percentage).\n",
        "\n",
        "**Pressure(in)**: Shows the air pressure (in inches).\n",
        "\n",
        "**Visibility(mi)**: Shows visibility (in miles).\n",
        "\n",
        "**Wind_Direction**: Shows wind direction.\n",
        "\n",
        "**Wind_Speed(mph)**: Shows wind speed (in miles per hour).\n",
        "\n",
        "**Precipitation(in)**: Shows precipitation amount in inches, if there is any.\n",
        "\n",
        "**Weather_Condition**: Shows the weather condition (rain, snow, thunderstorm, fog, etc.)\n",
        "\n",
        "**Amenity**: A POI annotation which indicates presence of amenity in a nearby location.\n",
        "\n",
        "**Bump**: A POI annotation which indicates presence of speed bump or hump in a nearby location.\n",
        "\n",
        "**Crossing**: A POI annotation which indicates presence of crossing in a nearby location.\n",
        "\n",
        "**Give_Way**: A POI annotation which indicates presence of give_way in a nearby location.\n",
        "\n",
        "**Junction**: A POI annotation which indicates presence of junction in a nearby location.\n",
        "\n",
        "**No_Exit**: A POI annotation which indicates presence of no_exit in a nearby location.\n",
        "\n",
        "**Railway**: A POI annotation which indicates presence of railway in a nearby location.\n",
        "\n",
        "**Roundabout**: A POI annotation which indicates presence of roundabout in a nearby location.\n",
        "\n",
        "**Station**: A POI annotation which indicates presence of station in a nearby location.\n",
        "\n",
        "**Stop**: A POI annotation which indicates presence of stop in a nearby location.\n",
        "\n",
        "**Traffic_Calming**: A POI annotation which indicates presence of traffic_calming in a nearby location.\n",
        "\n",
        "**Traffic_Signal**: A POI annotation which indicates presence of traffic_signal in a nearby location.\n",
        "\n",
        "**Turning_Loop**: A POI annotation which indicates presence of turning_loop in a nearby location.\n",
        "\n",
        "**Sunrise_Sunset**: Shows the period of day (i.e. day or night) based on sunrise/sunset.\n",
        "\n",
        "**Civil_Twilight**: Shows the period of day (i.e. day or night) based on civil twilight.\n",
        "\n",
        "**Nautical_Twilight**: Shows the period of day (i.e. day or night) based on nautical twilight.\n",
        "\n",
        "**Astronomical_Twilight**: Shows the period of day (i.e. day or night) based on astronomical twilight."
      ],
      "metadata": {
        "id": "OrkbYNWmuQEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Libraries"
      ],
      "metadata": {
        "id": "Popope_luQEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evalml"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T19:29:38.244178Z",
          "iopub.execute_input": "2023-09-19T19:29:38.245374Z",
          "iopub.status.idle": "2023-09-19T19:29:41.525280Z",
          "shell.execute_reply.started": "2023-09-19T19:29:38.245310Z",
          "shell.execute_reply": "2023-09-19T19:29:41.524133Z"
        },
        "trusted": true,
        "id": "4WS2OdGnuQEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycaret"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:45:41.301469Z",
          "iopub.execute_input": "2023-09-19T15:45:41.301891Z",
          "iopub.status.idle": "2023-09-19T15:46:02.921002Z",
          "shell.execute_reply.started": "2023-09-19T15:45:41.301852Z",
          "shell.execute_reply": "2023-09-19T15:46:02.919581Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "t1pypXiwuQEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install h2o"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:46:02.922986Z",
          "iopub.execute_input": "2023-09-19T15:46:02.923365Z",
          "iopub.status.idle": "2023-09-19T15:46:17.543348Z",
          "shell.execute_reply.started": "2023-09-19T15:46:02.923330Z",
          "shell.execute_reply": "2023-09-19T15:46:17.541506Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "229J7NIluQEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tpot"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:46:17.548331Z",
          "iopub.execute_input": "2023-09-19T15:46:17.548841Z",
          "iopub.status.idle": "2023-09-19T15:46:32.444633Z",
          "shell.execute_reply.started": "2023-09-19T15:46:17.548794Z",
          "shell.execute_reply": "2023-09-19T15:46:32.443212Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "pvGg-L-muQEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:46:32.446802Z",
          "iopub.execute_input": "2023-09-19T15:46:32.447179Z",
          "iopub.status.idle": "2023-09-19T15:46:50.429309Z",
          "shell.execute_reply.started": "2023-09-19T15:46:32.447144Z",
          "shell.execute_reply": "2023-09-19T15:46:50.428245Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "Q5oMKAJ7uQEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Necessary Libraries**"
      ],
      "metadata": {
        "id": "h85kOEC9uQEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import h2o\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "from shapely.geometry import Polygon\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# from evalml.automl import AutoMLSearch\n",
        "# from pycaret.classification import setup, compare_models, predict_model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# from evalml.objectives import get_optimization_objectives\n",
        "# from evalml.problem_types import ProblemTypes\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.ensemble import BalancedBaggingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from h2o.automl import H2OAutoML\n",
        "from tpot import TPOTClassifier\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words=stopwords.words(\"english\")\n",
        "new_stopping_words = stop_words[:len(stop_words)-36]\n",
        "new_stopping_words.remove(\"not\")\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:57:54.652826Z",
          "iopub.execute_input": "2023-09-19T17:57:54.653252Z",
          "iopub.status.idle": "2023-09-19T17:57:54.673976Z",
          "shell.execute_reply.started": "2023-09-19T17:57:54.653222Z",
          "shell.execute_reply": "2023-09-19T17:57:54.672749Z"
        },
        "trusted": true,
        "id": "ncVsUnWSuQEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Data Discovery**"
      ],
      "metadata": {
        "id": "5e-9nOvRuQEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/us-accidents/US_Accidents_March23.csv\" , sep = \",\" , encoding = \"utf-8\")\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:57:57.556568Z",
          "iopub.execute_input": "2023-09-19T17:57:57.557523Z",
          "iopub.status.idle": "2023-09-19T18:00:11.387894Z",
          "shell.execute_reply.started": "2023-09-19T17:57:57.557475Z",
          "shell.execute_reply": "2023-09-19T18:00:11.385184Z"
        },
        "trusted": true,
        "id": "xLyJSucWuQEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#showing type of every column\n",
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:00:11.395122Z",
          "iopub.execute_input": "2023-09-19T18:00:11.396346Z",
          "iopub.status.idle": "2023-09-19T18:00:11.459801Z",
          "shell.execute_reply.started": "2023-09-19T18:00:11.396134Z",
          "shell.execute_reply": "2023-09-19T18:00:11.455937Z"
        },
        "trusted": true,
        "id": "KJmwJJpiuQEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "ULvbO-s4uQEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning"
      ],
      "metadata": {
        "id": "2kxLDUSduQEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#searching about duplicated values\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:48:42.768876Z",
          "iopub.execute_input": "2023-09-19T15:48:42.769370Z",
          "iopub.status.idle": "2023-09-19T15:50:09.729180Z",
          "shell.execute_reply.started": "2023-09-19T15:48:42.769326Z",
          "shell.execute_reply": "2023-09-19T15:50:09.727947Z"
        },
        "trusted": true,
        "id": "SbXRSPoSuQEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There is no duplicated data"
      ],
      "metadata": {
        "id": "d8enB8kauQEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(thresh=40)\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:00:11.464166Z",
          "iopub.execute_input": "2023-09-19T18:00:11.465619Z",
          "iopub.status.idle": "2023-09-19T18:00:42.539712Z",
          "shell.execute_reply.started": "2023-09-19T18:00:11.465455Z",
          "shell.execute_reply": "2023-09-19T18:00:42.538075Z"
        },
        "trusted": true,
        "id": "Uh-XEvMhuQEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filling Missing Values"
      ],
      "metadata": {
        "id": "dU5BTDuHuQEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:51:08.489298Z",
          "iopub.execute_input": "2023-09-19T15:51:08.489657Z",
          "iopub.status.idle": "2023-09-19T15:51:56.966265Z",
          "shell.execute_reply.started": "2023-09-19T15:51:08.489624Z",
          "shell.execute_reply": "2023-09-19T15:51:56.964958Z"
        },
        "trusted": true,
        "id": "QkuISpRCuQEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filling missing values with interpolate method\n",
        "# limit is Maximum number of consecutive NaNs to fill. Must be greater than 0.\n",
        "df.fillna(method='ffill', limit=5, inplace=True)\n",
        "df.fillna(method='bfill', limit=5, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:00:42.544172Z",
          "iopub.execute_input": "2023-09-19T18:00:42.544725Z",
          "iopub.status.idle": "2023-09-19T18:01:21.805707Z",
          "shell.execute_reply.started": "2023-09-19T18:00:42.544674Z",
          "shell.execute_reply": "2023-09-19T18:01:21.804392Z"
        },
        "trusted": true,
        "id": "HfytmgHwuQEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:53:41.854776Z",
          "iopub.execute_input": "2023-09-19T15:53:41.855235Z",
          "iopub.status.idle": "2023-09-19T15:54:30.434809Z",
          "shell.execute_reply.started": "2023-09-19T15:53:41.855183Z",
          "shell.execute_reply": "2023-09-19T15:54:30.433653Z"
        },
        "trusted": true,
        "id": "OihcEe-BuQEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['End_Lat', 'End_Lng'],inplace= True)\n",
        "df.dropna(subset=['Wind_Chill(F)','Wind_Speed(mph)','Precipitation(in)'],inplace = True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:01:21.807275Z",
          "iopub.execute_input": "2023-09-19T18:01:21.807885Z",
          "iopub.status.idle": "2023-09-19T18:01:29.632495Z",
          "shell.execute_reply.started": "2023-09-19T18:01:21.807830Z",
          "shell.execute_reply": "2023-09-19T18:01:29.631122Z"
        },
        "trusted": true,
        "id": "rb5h_DA6uQEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:54:43.931745Z",
          "iopub.execute_input": "2023-09-19T15:54:43.932688Z",
          "iopub.status.idle": "2023-09-19T15:54:43.941007Z",
          "shell.execute_reply.started": "2023-09-19T15:54:43.932638Z",
          "shell.execute_reply": "2023-09-19T15:54:43.939847Z"
        },
        "trusted": true,
        "id": "5y5sPL82uQEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:54:43.942664Z",
          "iopub.execute_input": "2023-09-19T15:54:43.943829Z",
          "iopub.status.idle": "2023-09-19T15:55:27.341694Z",
          "shell.execute_reply.started": "2023-09-19T15:54:43.943798Z",
          "shell.execute_reply": "2023-09-19T15:55:27.340502Z"
        },
        "trusted": true,
        "id": "O6UtbmwOuQEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outliers"
      ],
      "metadata": {
        "id": "JohDkAnSuQEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:55:27.343434Z",
          "iopub.execute_input": "2023-09-19T15:55:27.343844Z",
          "iopub.status.idle": "2023-09-19T15:55:30.862383Z",
          "shell.execute_reply.started": "2023-09-19T15:55:27.343809Z",
          "shell.execute_reply": "2023-09-19T15:55:30.861461Z"
        },
        "trusted": true,
        "id": "Lh3IPMg_uQEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " By comparing mean with max && mean with std we got that:\n",
        "  *  In Start_Lat column, std is greater than mean --> outlier.\n",
        "  *  In Start_Lng column, std is greater than mean --> outlier."
      ],
      "metadata": {
        "id": "cyRhw5QGuQEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OutliersColumns = [\"Start_Lat\",\"Start_Lng\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:55:30.863854Z",
          "iopub.execute_input": "2023-09-19T15:55:30.864187Z",
          "iopub.status.idle": "2023-09-19T15:55:30.868709Z",
          "shell.execute_reply.started": "2023-09-19T15:55:30.864159Z",
          "shell.execute_reply": "2023-09-19T15:55:30.867764Z"
        },
        "trusted": true,
        "id": "N5CtIy7huQEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in OutliersColumns:\n",
        "#   # IQR\n",
        "#   # Calculate the upper and lower limits\n",
        "#   Q1 = df[i].quantile(0.25)\n",
        "#   Q3 = df[i].quantile(0.75)\n",
        "#   IQR = Q3 - Q1\n",
        "#   lower = Q1 - 1.5*IQR\n",
        "#   upper = Q3 + 1.5*IQR\n",
        "\n",
        "#   # Create arrays of Boolean values indicating the outlier rows\n",
        "#   upper_array = np.where(df[i]>=upper)[0]\n",
        "#   lower_array = np.where(df[i]<=lower)[0]\n",
        "\n",
        "#   # Removing the outliers\n",
        "#   df = df[~df.index.isin(upper_array)]\n",
        "#   df = df[~df.index.isin(lower_array)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:55:30.870221Z",
          "iopub.execute_input": "2023-09-19T15:55:30.870731Z",
          "iopub.status.idle": "2023-09-19T15:55:30.892379Z",
          "shell.execute_reply.started": "2023-09-19T15:55:30.870701Z",
          "shell.execute_reply": "2023-09-19T15:55:30.891212Z"
        },
        "trusted": true,
        "id": "U39d2Z75uQEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_dff = df.sample(frac=0.1, random_state=42)\n",
        "sampled_dff.dropna(inplace=True)\n",
        "categorical_cols = sampled_dff.select_dtypes(include=['object','bool','category']).columns\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in categorical_cols:\n",
        "    sampled_dff[col] = label_encoder.fit_transform(sampled_dff[col])\n",
        "\n",
        "X = sampled_dff.drop('Severity', axis=1)\n",
        "y= sampled_dff['Severity']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# print(classification_task(rf_classifier,X_train, y_train ,X_test,y_test, y_pred,'rf'))\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:55:30.894310Z",
          "iopub.execute_input": "2023-09-19T15:55:30.894775Z",
          "iopub.status.idle": "2023-09-19T16:00:45.569312Z",
          "shell.execute_reply.started": "2023-09-19T15:55:30.894732Z",
          "shell.execute_reply": "2023-09-19T16:00:45.568008Z"
        },
        "trusted": true,
        "id": "Bv689x-WuQEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "dUkDelZFuQEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering is a critical step in the process of data analysis and machine learning, where we transform raw data into meaningful and informative features that can enhance the performance of predictive models. In the context of the US Accidents dataset, effective feature engineering can significantly improve our understanding of accident patterns, help us build more accurate predictive models, and ultimately contribute to safer roads and transportation systems.\n",
        "\n",
        "The US Accidents dataset provides us with a wealth of information about traffic accidents across the United States. However, the raw data often contains numerous variables that may not be directly suitable for modeling. This is where feature engineering comes into play.\n",
        "\n",
        "In this section, we will explore various techniques and considerations for feature engineering specific to the US Accidents dataset. Our goal is to derive new features or transform existing ones that capture the underlying patterns, relationships, and factors that influence accidents."
      ],
      "metadata": {
        "id": "1uLAconCuQEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping Unncessary Columns"
      ],
      "metadata": {
        "id": "sSwisEweuQEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping unnecessary columns or features from a dataset is a common practice for several reasons:\n",
        "\n",
        "1. **Dimensionality Reduction:** Including a large number of unnecessary or irrelevant features can lead to high dimensionality in your dataset. High-dimensional data can be computationally expensive to work with and can increase the risk of overfitting, where a model learns noise in the data rather than true patterns. By removing unnecessary columns, you can reduce the dimensionality of your dataset, which can lead to more efficient and interpretable models.\n",
        "\n",
        "2. **Noise Reduction:** Some columns in a dataset may contain noisy or irrelevant information that does not contribute to the predictive power of a model. These noisy features can introduce randomness and make it harder for a model to generalize well to new data. By eliminating such features, you can improve the signal-to-noise ratio in your data and enhance model performance.\n",
        "\n",
        "3. **Simplification and Interpretability:** Removing unnecessary columns can simplify the modeling process and make the model more interpretable. A simpler model is often easier to understand, explain, and maintain. It can also help in identifying the most important factors contributing to the outcome.\n",
        "\n",
        "4. **Faster Training and Inference:** Smaller datasets with fewer features typically require less computational resources and time for model training and inference. This can be crucial in situations where efficiency is a concern, such as real-time applications or working with large datasets.\n",
        "\n",
        "In summary, dropping unnecessary columns during feature engineering is a strategy to improve the quality of your data and the efficiency of your modeling process. However, this should be done judiciously and in consideration of the specific characteristics of your dataset and the goals of your analysis."
      ],
      "metadata": {
        "id": "vZ5oIZIjuQEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- After careful observation the following insights were observed:\n",
        "\n",
        "1- **Description, ID, Country, Turning loop, Airport_Code, ZipCode** will be dropped because these columns will not provide anything informative to the model"
      ],
      "metadata": {
        "id": "U7MUdgEwuQEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2= df.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:01:29.634388Z",
          "iopub.execute_input": "2023-09-19T18:01:29.635617Z",
          "iopub.status.idle": "2023-09-19T18:01:37.371827Z",
          "shell.execute_reply.started": "2023-09-19T18:01:29.635579Z",
          "shell.execute_reply": "2023-09-19T18:01:37.370497Z"
        },
        "trusted": true,
        "id": "dOfatB96uQEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df2.drop(columns =['Description', 'ID', 'Country','Turning_Loop','Airport_Code','Zipcode'],inplace = True)\n",
        "df2.drop(columns =['Description','Country','Turning_Loop','ID'],inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:01:37.373522Z",
          "iopub.execute_input": "2023-09-19T18:01:37.373968Z",
          "iopub.status.idle": "2023-09-19T18:01:39.650882Z",
          "shell.execute_reply.started": "2023-09-19T18:01:37.373935Z",
          "shell.execute_reply": "2023-09-19T18:01:39.649642Z"
        },
        "trusted": true,
        "id": "Dse4o0cwuQEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_dff = df2.sample(frac=0.1, random_state=42)\n",
        "\n",
        "sampled_dff.dropna(inplace=True)\n",
        "\n",
        "categorical_cols = sampled_dff.select_dtypes(include=['object','bool','category']).columns\n",
        "label_encoder = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    sampled_dff[col] = label_encoder.fit_transform(sampled_dff[col])\n",
        "\n",
        "X = sampled_dff.drop('Severity', axis=1)\n",
        "y= sampled_dff['Severity']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# print(classification_task(rf_classifier,X_train, y_train ,X_test,y_test, y_pred,'rf'))\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:00:52.318617Z",
          "iopub.execute_input": "2023-09-19T16:00:52.319136Z",
          "iopub.status.idle": "2023-09-19T16:05:48.500074Z",
          "shell.execute_reply.started": "2023-09-19T16:00:52.319090Z",
          "shell.execute_reply": "2023-09-19T16:05:48.498786Z"
        },
        "trusted": true,
        "id": "Jz9UH30KuQEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Creation"
      ],
      "metadata": {
        "id": "cVq6eGPcuQEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binning"
      ],
      "metadata": {
        "id": "suLNAFT4uQEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binning is the process of categorizing continuous numerical data into discrete intervals or \"bins.\" This transformation serves several essential purposes in data preprocessing and analysis. Binning not only simplifies complex datasets but also helps uncover patterns, enhances model interpretability, and can be particularly valuable when dealing with machine learning algorithms that perform better with categorical or discrete features.\n",
        "\n",
        "Here are some key reasons why we employ binning in feature engineering:\n",
        "\n",
        "1. **Simplification of Complexity**: Continuous numerical features, such as age, income, or temperature, can introduce complexity to machine learning models. Binning allows us to group these continuous values into meaningful categories, making it easier for models to capture relationships.\n",
        "\n",
        "2. **Feature Interpretability**: Discretized features are often more interpretable than continuous ones. They provide a clear structure that allows domain experts and stakeholders to understand how different feature values relate to the target variable.\n",
        "\n",
        "3. **Model Performance**: Binning can enhance the performance of certain machine learning algorithms, especially decision tree-based models, by creating splits that improve prediction accuracy.\n",
        "\n",
        "7. **Reducing Overfitting**: Binning can help reduce overfitting by simplifying the feature space and preventing the model from fitting noise in the data.\n",
        "\n",
        "In summary, binning is a crucial technique in feature engineering that transforms continuous numerical data into discrete categories or intervals. This process simplifies data, makes relationships more interpretable, and can improve the performance of machine learning models, particularly when dealing with non-linear or complex datasets. However, it's essential to choose the right binning strategy and the number of bins carefully to ensure that the resulting features effectively capture the underlying patterns in the data."
      ],
      "metadata": {
        "id": "cYQaf5X5uQEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = df2.select_dtypes(include=['object'])\n",
        "categorical_cols.nunique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:05:48.502146Z",
          "iopub.execute_input": "2023-09-19T16:05:48.502612Z",
          "iopub.status.idle": "2023-09-19T16:06:09.795879Z",
          "shell.execute_reply.started": "2023-09-19T16:05:48.502565Z",
          "shell.execute_reply": "2023-09-19T16:06:09.794684Z"
        },
        "trusted": true,
        "id": "SkWPoTzPuQEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Since that most categorical columns have too many unique values , we will need binning . Therefore, we will have to create extra columns or overwrite the columns which have unique values > 20. Also we will only bin the categorical variables that have feature importance"
      ],
      "metadata": {
        "id": "323Aasm9uQEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Weather_Condition'].unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:06:09.797656Z",
          "iopub.execute_input": "2023-09-19T16:06:09.798391Z",
          "iopub.status.idle": "2023-09-19T16:06:10.363063Z",
          "shell.execute_reply.started": "2023-09-19T16:06:09.798348Z",
          "shell.execute_reply": "2023-09-19T16:06:10.361877Z"
        },
        "trusted": true,
        "id": "LLwX0JgzuQEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_bins = {\n",
        "    'Clear': ['Clear', 'Fair'],\n",
        "    'Cloudy': ['Cloudy', 'Mostly Cloudy', 'Partly Cloudy', 'Scattered Clouds'],\n",
        "    'Rainy': ['Light Rain', 'Rain', 'Light Freezing Drizzle', 'Light Drizzle', 'Heavy Rain', 'Light Freezing Rain', 'Drizzle', 'Light Freezing Fog', 'Light Rain Showers', 'Showers in the Vicinity', 'T-Storm', 'Thunder', 'Patches of Fog', 'Heavy T-Storm', 'Heavy Thunderstorms and Rain', 'Funnel Cloud', 'Heavy T-Storm / Windy', 'Heavy Thunderstorms and Snow', 'Rain / Windy', 'Heavy Rain / Windy', 'Squalls', 'Heavy Ice Pellets', 'Thunder / Windy', 'Drizzle and Fog', 'T-Storm / Windy', 'Smoke / Windy', 'Haze / Windy', 'Light Drizzle / Windy', 'Widespread Dust / Windy', 'Wintry Mix', 'Wintry Mix / Windy', 'Light Snow with Thunder', 'Fog / Windy', 'Snow and Thunder', 'Sleet / Windy', 'Heavy Freezing Rain / Windy', 'Squalls / Windy', 'Light Rain Shower / Windy', 'Snow and Thunder / Windy', 'Light Sleet / Windy', 'Sand / Dust Whirlwinds', 'Mist / Windy', 'Drizzle / Windy', 'Duststorm', 'Sand / Dust Whirls Nearby', 'Thunder and Hail', 'Freezing Rain / Windy', 'Light Snow Shower / Windy', 'Partial Fog', 'Thunder / Wintry Mix / Windy', 'Patches of Fog / Windy', 'Rain and Sleet', 'Light Snow Grains', 'Partial Fog / Windy', 'Sand / Dust Whirlwinds / Windy', 'Heavy Snow with Thunder', 'Heavy Blowing Snow', 'Low Drifting Snow', 'Light Hail', 'Light Thunderstorm', 'Heavy Freezing Drizzle', 'Light Blowing Snow', 'Thunderstorms and Snow', 'Heavy Rain Showers', 'Rain Shower / Windy', 'Sleet and Thunder', 'Heavy Sleet and Thunder', 'Drifting Snow / Windy', 'Shallow Fog / Windy', 'Thunder and Hail / Windy', 'Heavy Sleet / Windy', 'Sand / Windy', 'Heavy Rain Shower / Windy', 'Blowing Snow Nearby', 'Blowing Sand', 'Heavy Rain Shower', 'Drifting Snow', 'Heavy Thunderstorms with Small Hail'],\n",
        "    'Snowy': ['Light Snow', 'Snow', 'Light Snow / Windy', 'Snow Grains', 'Snow Showers', 'Snow / Windy', 'Light Snow and Sleet', 'Snow and Sleet', 'Light Snow and Sleet / Windy', 'Snow and Sleet / Windy'],\n",
        "    'Windy': ['Blowing Dust / Windy', 'Fair / Windy', 'Mostly Cloudy / Windy', 'Light Rain / Windy', 'T-Storm / Windy', 'Blowing Snow / Windy', 'Freezing Rain / Windy', 'Light Snow and Sleet / Windy', 'Sleet and Thunder / Windy', 'Blowing Snow Nearby', 'Heavy Rain Shower / Windy'],\n",
        "    'Hail': ['Hail'],\n",
        "    'Volcanic Ash': ['Volcanic Ash'],\n",
        "    'Tornado': ['Tornado']\n",
        "}\n",
        "\n",
        "def map_weather_to_bins(weather):\n",
        "    for bin_name, bin_values in weather_bins.items():\n",
        "        if weather in bin_values:\n",
        "            return bin_name\n",
        "    return 'Other'\n",
        "\n",
        "df2['Weather_Bin'] = df2['Weather_Condition'].apply(map_weather_to_bins)\n",
        "df2['Weather_Bin']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:01:39.652791Z",
          "iopub.execute_input": "2023-09-19T18:01:39.653950Z",
          "iopub.status.idle": "2023-09-19T18:01:44.626731Z",
          "shell.execute_reply.started": "2023-09-19T18:01:39.653908Z",
          "shell.execute_reply": "2023-09-19T18:01:44.625388Z"
        },
        "trusted": true,
        "id": "uB51yqAAuQEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.drop(columns=['Weather_Condition'], inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:01:44.628592Z",
          "iopub.execute_input": "2023-09-19T18:01:44.629524Z",
          "iopub.status.idle": "2023-09-19T18:01:46.809078Z",
          "shell.execute_reply.started": "2023-09-19T18:01:44.629478Z",
          "shell.execute_reply": "2023-09-19T18:01:46.808228Z"
        },
        "trusted": true,
        "id": "kM7gkIauuQEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Street'].unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:06:17.984122Z",
          "iopub.execute_input": "2023-09-19T16:06:17.984525Z",
          "iopub.status.idle": "2023-09-19T16:06:18.972833Z",
          "shell.execute_reply.started": "2023-09-19T16:06:17.984490Z",
          "shell.execute_reply": "2023-09-19T16:06:18.971997Z"
        },
        "trusted": true,
        "id": "QlE-9NWXuQEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df2['Temperature_Category'] = pd.cut(df2['Temperature(F)'], bins=[-100, 50, 80, 200], labels=['Cold', 'Mild', 'Hot'])\n",
        "# df2['Humidity_Level'] = pd.cut(df2['Humidity(%)'], bins=[0, 30, 70, 100], labels=['Low', 'Moderate', 'High'])\n",
        "\n",
        "df2['Pressure_Category'] = pd.cut(df2['Pressure(in)'], bins=[0, 29.5, 30.2, 100], labels=['Low', 'Normal', 'High'])\n",
        "df2['Visibility_Category'] = pd.cut(df2['Visibility(mi)'], bins=[0, 1, 5, 100], labels=['Poor', 'Moderate', 'Clear'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:01:46.812528Z",
          "iopub.execute_input": "2023-09-19T18:01:46.812919Z",
          "iopub.status.idle": "2023-09-19T18:01:47.074027Z",
          "shell.execute_reply.started": "2023-09-19T18:01:46.812886Z",
          "shell.execute_reply": "2023-09-19T18:01:47.072827Z"
        },
        "trusted": true,
        "id": "PntUZjYLuQEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.drop(columns=['Pressure(in)','Visibility(mi)'], inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:01:47.075845Z",
          "iopub.execute_input": "2023-09-19T18:01:47.076229Z",
          "iopub.status.idle": "2023-09-19T18:01:49.183938Z",
          "shell.execute_reply.started": "2023-09-19T18:01:47.076195Z",
          "shell.execute_reply": "2023-09-19T18:01:49.182606Z"
        },
        "trusted": true,
        "id": "tZ6ACVV7uQEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['State'].unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:06:21.309811Z",
          "iopub.execute_input": "2023-09-19T16:06:21.310951Z",
          "iopub.status.idle": "2023-09-19T16:06:21.805839Z",
          "shell.execute_reply.started": "2023-09-19T16:06:21.310858Z",
          "shell.execute_reply": "2023-09-19T16:06:21.804612Z"
        },
        "trusted": true,
        "id": "TDsJSL_NuQEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['City'].nunique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:06:21.807539Z",
          "iopub.execute_input": "2023-09-19T16:06:21.808048Z",
          "iopub.status.idle": "2023-09-19T16:06:22.505031Z",
          "shell.execute_reply.started": "2023-09-19T16:06:21.808005Z",
          "shell.execute_reply": "2023-09-19T16:06:22.503691Z"
        },
        "trusted": true,
        "id": "gLiUQLPeuQEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['County'].nunique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:06:22.506681Z",
          "iopub.execute_input": "2023-09-19T16:06:22.507250Z",
          "iopub.status.idle": "2023-09-19T16:06:23.113644Z",
          "shell.execute_reply.started": "2023-09-19T16:06:22.507206Z",
          "shell.execute_reply": "2023-09-19T16:06:23.112402Z"
        },
        "trusted": true,
        "id": "MbgBK_TNuQEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ],
      "metadata": {
        "id": "zJfHloqFuQEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering geographical coordinates, often represented by latitude and longitude values, is a valuable data analysis technique with a wide range of practical applications. It involves grouping data points (locations) based on their proximity in physical space. The primary motivation for clustering latitude and longitude coordinates lies in its ability to reveal meaningful patterns and insights from spatial data that have a lot of unique values .\n",
        "\n",
        "In essence, clustering latitude and longitude coordinates is a powerful technique that transforms geographical data into actionable insights. It allows us to group locations with similar spatial characteristics."
      ],
      "metadata": {
        "id": "KeCJTdGDuQEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df2[['Start_Lat', 'Start_Lng']]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:01:49.188372Z",
          "iopub.execute_input": "2023-09-19T18:01:49.189271Z",
          "iopub.status.idle": "2023-09-19T18:01:49.365028Z",
          "shell.execute_reply.started": "2023-09-19T18:01:49.189231Z",
          "shell.execute_reply": "2023-09-19T18:01:49.363746Z"
        },
        "trusted": true,
        "id": "WVZyoWzpuQEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wcss = []\n",
        "max_clusters = 10\n",
        "for i in range(1, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, max_clusters + 1), wcss, marker='o', linestyle='--')\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('WCSS')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:06:23.115203Z",
          "iopub.execute_input": "2023-09-19T16:06:23.115664Z",
          "iopub.status.idle": "2023-09-19T16:10:35.117304Z",
          "shell.execute_reply.started": "2023-09-19T16:06:23.115632Z",
          "shell.execute_reply": "2023-09-19T16:10:35.116000Z"
        },
        "trusted": true,
        "id": "qmRWGJd0uQEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "df2['cluster_LatLng'] = clusters"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:01:49.369255Z",
          "iopub.execute_input": "2023-09-19T18:01:49.370461Z",
          "iopub.status.idle": "2023-09-19T18:02:13.114110Z",
          "shell.execute_reply.started": "2023-09-19T18:01:49.370409Z",
          "shell.execute_reply": "2023-09-19T18:02:13.113179Z"
        },
        "trusted": true,
        "id": "mCMaCXvuuQEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df2['Start_Lat'], df2['Start_Lng'], c=df2['cluster_LatLng'], cmap='rainbow')\n",
        "plt.title('K-Means Clustering based on Lat and Lng')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:10:35.119051Z",
          "iopub.execute_input": "2023-09-19T16:10:35.119592Z",
          "iopub.status.idle": "2023-09-19T16:13:10.721375Z",
          "shell.execute_reply.started": "2023-09-19T16:10:35.119532Z",
          "shell.execute_reply": "2023-09-19T16:13:10.720369Z"
        },
        "trusted": true,
        "id": "vUH4k_t7uQEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['cluster_LatLng'].unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:13:10.723071Z",
          "iopub.execute_input": "2023-09-19T16:13:10.723717Z",
          "iopub.status.idle": "2023-09-19T16:13:10.764196Z",
          "shell.execute_reply.started": "2023-09-19T16:13:10.723682Z",
          "shell.execute_reply": "2023-09-19T16:13:10.762976Z"
        },
        "trusted": true,
        "id": "fb4reQsbuQEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can now remove lat and lng after we clustered the lat and lng"
      ],
      "metadata": {
        "id": "4spn35LTuQEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.drop(columns=['Start_Lat','Start_Lng'],inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:02:13.115594Z",
          "iopub.execute_input": "2023-09-19T18:02:13.116599Z",
          "iopub.status.idle": "2023-09-19T18:02:15.241649Z",
          "shell.execute_reply.started": "2023-09-19T18:02:13.116553Z",
          "shell.execute_reply": "2023-09-19T18:02:15.240356Z"
        },
        "trusted": true,
        "id": "CcME4-jguQEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "\n",
        "X = label_encoder.fit_transform(df2['Zipcode']).reshape(-1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:02:15.243389Z",
          "iopub.execute_input": "2023-09-19T18:02:15.244046Z",
          "iopub.status.idle": "2023-09-19T18:02:23.779101Z",
          "shell.execute_reply.started": "2023-09-19T18:02:15.244004Z",
          "shell.execute_reply": "2023-09-19T18:02:23.777823Z"
        },
        "trusted": true,
        "id": "d1lMwRTvuQEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "wcss = []\n",
        "max_clusters = 10\n",
        "for i in range(1, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, max_clusters + 1), wcss, marker='o', linestyle='--')\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('WCSS')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:13:13.056448Z",
          "iopub.execute_input": "2023-09-19T16:13:13.056823Z",
          "iopub.status.idle": "2023-09-19T16:17:28.348709Z",
          "shell.execute_reply.started": "2023-09-19T16:13:13.056791Z",
          "shell.execute_reply": "2023-09-19T16:17:28.347250Z"
        },
        "trusted": true,
        "id": "y91mYzhpuQEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:02:23.780579Z",
          "iopub.execute_input": "2023-09-19T18:02:23.781072Z",
          "iopub.status.idle": "2023-09-19T18:02:35.624706Z",
          "shell.execute_reply.started": "2023-09-19T18:02:23.781026Z",
          "shell.execute_reply": "2023-09-19T18:02:35.623512Z"
        },
        "trusted": true,
        "id": "W6LIhjmKuQEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 0], s = 100, c = 'red', label = 'Cluster 1')\n",
        "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 0], s = 100, c = 'blue', label = 'Cluster 2')\n",
        "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 0], s = 100, c = 'green', label = 'Cluster 3')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 0], s = 300, c = 'yellow', label = 'Centroids')\n",
        "\n",
        "plt.title('Clusters of Zipcode')\n",
        "# plt.xlabel('Severity')\n",
        "# plt.ylabel('ZipCode')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:17:28.350427Z",
          "iopub.execute_input": "2023-09-19T16:17:28.350778Z",
          "iopub.status.idle": "2023-09-19T16:19:12.236163Z",
          "shell.execute_reply.started": "2023-09-19T16:17:28.350747Z",
          "shell.execute_reply": "2023-09-19T16:19:12.234694Z"
        },
        "trusted": true,
        "id": "SyjpBNKFuQEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['cluster_Zipcode'] = y_kmeans\n",
        "df2['cluster_Zipcode'].unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:02:35.626202Z",
          "iopub.execute_input": "2023-09-19T18:02:35.627199Z",
          "iopub.status.idle": "2023-09-19T18:02:35.676669Z",
          "shell.execute_reply.started": "2023-09-19T18:02:35.627154Z",
          "shell.execute_reply": "2023-09-19T18:02:35.675470Z"
        },
        "trusted": true,
        "id": "bzLCuoSBuQEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X =label_encoder.fit_transform(df2['Airport_Code']).reshape(-1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:02:46.317841Z",
          "iopub.execute_input": "2023-09-19T18:02:46.318421Z",
          "iopub.status.idle": "2023-09-19T18:02:48.606707Z",
          "shell.execute_reply.started": "2023-09-19T18:02:46.318372Z",
          "shell.execute_reply": "2023-09-19T18:02:48.605404Z"
        },
        "trusted": true,
        "id": "Nl5RbPq-uQEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wcss = []\n",
        "max_clusters = 10\n",
        "for i in range(1, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, max_clusters + 1), wcss, marker='o', linestyle='--')\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('WCSS')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:19:12.288683Z",
          "iopub.execute_input": "2023-09-19T16:19:12.289169Z",
          "iopub.status.idle": "2023-09-19T16:23:14.033454Z",
          "shell.execute_reply.started": "2023-09-19T16:19:12.289126Z",
          "shell.execute_reply": "2023-09-19T16:23:14.032137Z"
        },
        "trusted": true,
        "id": "Oglr28q8uQEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:03:15.458682Z",
          "iopub.execute_input": "2023-09-19T18:03:15.459089Z",
          "iopub.status.idle": "2023-09-19T18:03:28.301952Z",
          "shell.execute_reply.started": "2023-09-19T18:03:15.459058Z",
          "shell.execute_reply": "2023-09-19T18:03:28.300689Z"
        },
        "trusted": true,
        "id": "OJxzux5luQEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 0], s = 100, c = 'red', label = 'Cluster 1')\n",
        "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 0], s = 100, c = 'blue', label = 'Cluster 2')\n",
        "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 0], s = 100, c = 'green', label = 'Cluster 3')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 0], s = 300, c = 'yellow', label = 'Centroids')\n",
        "\n",
        "plt.title('Clusters of Airport code')\n",
        "# plt.xlabel('Severity')\n",
        "# plt.ylabel('State')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:23:14.034902Z",
          "iopub.execute_input": "2023-09-19T16:23:14.035293Z",
          "iopub.status.idle": "2023-09-19T16:24:59.774287Z",
          "shell.execute_reply.started": "2023-09-19T16:23:14.035260Z",
          "shell.execute_reply": "2023-09-19T16:24:59.772985Z"
        },
        "trusted": true,
        "id": "3rNXDVFIuQEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['cluster_Airport_Code'] = y_kmeans\n",
        "df2['cluster_Airport_Code'].unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:03:28.303955Z",
          "iopub.execute_input": "2023-09-19T18:03:28.304420Z",
          "iopub.status.idle": "2023-09-19T18:03:28.354377Z",
          "shell.execute_reply.started": "2023-09-19T18:03:28.304374Z",
          "shell.execute_reply": "2023-09-19T18:03:28.352953Z"
        },
        "trusted": true,
        "id": "wbWzo-wtuQEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = label_encoder.fit_transform(df2['Street']).reshape(-1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:03:39.394500Z",
          "iopub.execute_input": "2023-09-19T18:03:39.394923Z",
          "iopub.status.idle": "2023-09-19T18:03:45.377161Z",
          "shell.execute_reply.started": "2023-09-19T18:03:39.394891Z",
          "shell.execute_reply": "2023-09-19T18:03:45.375711Z"
        },
        "trusted": true,
        "id": "pr9nojKhuQEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "wcss = []\n",
        "max_clusters = 10\n",
        "for i in range(1, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, max_clusters + 1), wcss, marker='o', linestyle='--')\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('WCSS')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:24:59.828870Z",
          "iopub.execute_input": "2023-09-19T16:24:59.829846Z",
          "iopub.status.idle": "2023-09-19T16:28:59.658268Z",
          "shell.execute_reply.started": "2023-09-19T16:24:59.829811Z",
          "shell.execute_reply": "2023-09-19T16:28:59.656738Z"
        },
        "trusted": true,
        "id": "T4pNHAhquQEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:04:01.436021Z",
          "iopub.execute_input": "2023-09-19T18:04:01.436450Z",
          "iopub.status.idle": "2023-09-19T18:04:12.780802Z",
          "shell.execute_reply.started": "2023-09-19T18:04:01.436416Z",
          "shell.execute_reply": "2023-09-19T18:04:12.776310Z"
        },
        "trusted": true,
        "id": "u_v8GuRbuQEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 0], s = 100, c = 'red', label = 'Cluster 1')\n",
        "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 0], s = 100, c = 'blue', label = 'Cluster 2')\n",
        "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 0], s = 100, c = 'green', label = 'Cluster 3')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 0], s = 300, c = 'yellow', label = 'Centroids')\n",
        "\n",
        "plt.title('Clusters of Street')\n",
        "# plt.xlabel('Severity')\n",
        "# plt.ylabel('Street')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:28:59.659943Z",
          "iopub.execute_input": "2023-09-19T16:28:59.660312Z",
          "iopub.status.idle": "2023-09-19T16:30:42.634228Z",
          "shell.execute_reply.started": "2023-09-19T16:28:59.660280Z",
          "shell.execute_reply": "2023-09-19T16:30:42.632887Z"
        },
        "trusted": true,
        "id": "0NUIOf3_uQEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['cluster_Street'] = y_kmeans\n",
        "df2['cluster_Street'].unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:04:12.782563Z",
          "iopub.execute_input": "2023-09-19T18:04:12.783729Z",
          "iopub.status.idle": "2023-09-19T18:04:12.847492Z",
          "shell.execute_reply.started": "2023-09-19T18:04:12.783679Z",
          "shell.execute_reply": "2023-09-19T18:04:12.846453Z"
        },
        "trusted": true,
        "id": "EvHVHUQ_uQEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = label_encoder.fit_transform(df2['City']).reshape(-1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:04:22.444419Z",
          "iopub.execute_input": "2023-09-19T18:04:22.444835Z",
          "iopub.status.idle": "2023-09-19T18:04:25.203388Z",
          "shell.execute_reply.started": "2023-09-19T18:04:22.444802Z",
          "shell.execute_reply": "2023-09-19T18:04:25.202139Z"
        },
        "trusted": true,
        "id": "ITzFyZ0SuQEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "wcss = []\n",
        "max_clusters = 10\n",
        "for i in range(1, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, max_clusters + 1), wcss, marker='o', linestyle='--')\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('WCSS')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:30:42.685883Z",
          "iopub.execute_input": "2023-09-19T16:30:42.686347Z",
          "iopub.status.idle": "2023-09-19T16:34:33.314319Z",
          "shell.execute_reply.started": "2023-09-19T16:30:42.686301Z",
          "shell.execute_reply": "2023-09-19T16:34:33.313430Z"
        },
        "trusted": true,
        "id": "27Bi3YjzuQEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:04:48.149927Z",
          "iopub.execute_input": "2023-09-19T18:04:48.150331Z",
          "iopub.status.idle": "2023-09-19T18:05:01.315919Z",
          "shell.execute_reply.started": "2023-09-19T18:04:48.150299Z",
          "shell.execute_reply": "2023-09-19T18:05:01.314882Z"
        },
        "trusted": true,
        "id": "j6FJTlizuQEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 0], s = 100, c = 'red', label = 'Cluster 1')\n",
        "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 0], s = 100, c = 'blue', label = 'Cluster 2')\n",
        "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 0], s = 100, c = 'green', label = 'Cluster 3')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 0], s = 300, c = 'yellow', label = 'Centroids')\n",
        "\n",
        "plt.title('Clusters of City')\n",
        "# plt.xlabel('Severity')\n",
        "# plt.ylabel('Street')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:34:33.316055Z",
          "iopub.execute_input": "2023-09-19T16:34:33.316713Z",
          "iopub.status.idle": "2023-09-19T16:36:16.222580Z",
          "shell.execute_reply.started": "2023-09-19T16:34:33.316679Z",
          "shell.execute_reply": "2023-09-19T16:36:16.221363Z"
        },
        "trusted": true,
        "id": "fWeU8N8ruQEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['cluster_City'] = y_kmeans\n",
        "df2['cluster_City'].unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:05:01.317706Z",
          "iopub.execute_input": "2023-09-19T18:05:01.318052Z",
          "iopub.status.idle": "2023-09-19T18:05:01.365874Z",
          "shell.execute_reply.started": "2023-09-19T18:05:01.318024Z",
          "shell.execute_reply": "2023-09-19T18:05:01.364654Z"
        },
        "trusted": true,
        "id": "sy4KO9dxuQEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = label_encoder.fit_transform(df2['County']).reshape(-1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:05:11.932245Z",
          "iopub.execute_input": "2023-09-19T18:05:11.932715Z",
          "iopub.status.idle": "2023-09-19T18:05:14.229038Z",
          "shell.execute_reply.started": "2023-09-19T18:05:11.932681Z",
          "shell.execute_reply": "2023-09-19T18:05:14.227754Z"
        },
        "trusted": true,
        "id": "ztt8JwGJuQEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "wcss = []\n",
        "max_clusters = 10\n",
        "for i in range(1, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, max_clusters + 1), wcss, marker='o', linestyle='--')\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('WCSS')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:36:16.276241Z",
          "iopub.execute_input": "2023-09-19T16:36:16.276578Z",
          "iopub.status.idle": "2023-09-19T16:40:00.418129Z",
          "shell.execute_reply.started": "2023-09-19T16:36:16.276548Z",
          "shell.execute_reply": "2023-09-19T16:40:00.416849Z"
        },
        "trusted": true,
        "id": "kmSdlaTJuQEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:05:30.635270Z",
          "iopub.execute_input": "2023-09-19T18:05:30.635852Z",
          "iopub.status.idle": "2023-09-19T18:05:42.143449Z",
          "shell.execute_reply.started": "2023-09-19T18:05:30.635807Z",
          "shell.execute_reply": "2023-09-19T18:05:42.142360Z"
        },
        "trusted": true,
        "id": "8-uJVSeBuQEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 0], s = 100, c = 'red', label = 'Cluster 1')\n",
        "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 0], s = 100, c = 'blue', label = 'Cluster 2')\n",
        "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 0], s = 100, c = 'green', label = 'Cluster 3')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 0], s = 300, c = 'yellow', label = 'Centroids')\n",
        "\n",
        "plt.title('Clusters of County')\n",
        "# plt.xlabel('Severity')\n",
        "# plt.ylabel('Street')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:40:00.419938Z",
          "iopub.execute_input": "2023-09-19T16:40:00.420807Z",
          "iopub.status.idle": "2023-09-19T16:41:44.234102Z",
          "shell.execute_reply.started": "2023-09-19T16:40:00.420762Z",
          "shell.execute_reply": "2023-09-19T16:41:44.232690Z"
        },
        "trusted": true,
        "id": "eYSUnkbXuQEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['cluster_County'] = y_kmeans\n",
        "df2['cluster_County'].unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:05:42.145981Z",
          "iopub.execute_input": "2023-09-19T18:05:42.146651Z",
          "iopub.status.idle": "2023-09-19T18:05:42.194674Z",
          "shell.execute_reply.started": "2023-09-19T18:05:42.146605Z",
          "shell.execute_reply": "2023-09-19T18:05:42.193593Z"
        },
        "trusted": true,
        "id": "CIgoe95_uQEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:41:44.289559Z",
          "iopub.execute_input": "2023-09-19T16:41:44.290525Z",
          "iopub.status.idle": "2023-09-19T16:41:44.298767Z",
          "shell.execute_reply.started": "2023-09-19T16:41:44.290481Z",
          "shell.execute_reply": "2023-09-19T16:41:44.297610Z"
        },
        "trusted": true,
        "id": "wD3jxtgRuQEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.drop(columns=['Zipcode','Airport_Code','Street','State','County'],inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T18:05:42.196122Z",
          "iopub.execute_input": "2023-09-19T18:05:42.196588Z",
          "iopub.status.idle": "2023-09-19T18:05:44.024828Z",
          "shell.execute_reply.started": "2023-09-19T18:05:42.196534Z",
          "shell.execute_reply": "2023-09-19T18:05:44.023786Z"
        },
        "trusted": true,
        "id": "DMDv_t47uQEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_dff = df2.sample(frac=0.1, random_state=42)\n",
        "sampled_dff.dropna(inplace=True)\n",
        "categorical_cols = sampled_dff.select_dtypes(include=['object','bool','category']).columns\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in categorical_cols:\n",
        "    sampled_dff[col] = label_encoder.fit_transform(sampled_dff[col])\n",
        "\n",
        "X = sampled_dff.drop('Severity', axis=1)\n",
        "y= sampled_dff['Severity']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# print(classification_task(rf_classifier,X_train, y_train ,X_test,y_test, y_pred,'rf'))\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:41:46.325712Z",
          "iopub.execute_input": "2023-09-19T16:41:46.326066Z",
          "iopub.status.idle": "2023-09-19T16:45:21.577703Z",
          "shell.execute_reply.started": "2023-09-19T16:41:46.326035Z",
          "shell.execute_reply": "2023-09-19T16:45:21.576460Z"
        },
        "trusted": true,
        "id": "lDIyrRysuQEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting Date and timestamps to Date-time\n",
        "\n",
        "\n",
        "DateTime data is prevalent in various datasets, representing timestamps, dates, or time intervals. While DateTime information is essential for many analyses, it often contains rich contextual details that can be challenging for machine learning models to interpret directly. To address this, feature extraction from DateTime types involves breaking down these timestamps into more understandable and informative components, such as year, month, day, hour, minute, and so on.\n",
        "\n",
        "The motivation behind extracting features like month, day, and year from DateTime types is multi-fold:\n",
        "\n",
        "1. **Temporal Patterns**: Extracting individual components allows machine learning models to capture temporal patterns and trends more effectively. For instance, identifying the day of the week or month of the year can be crucial in understanding weekly or seasonal variations in data.\n",
        "\n",
        "2. **Reducing Dimensionality**: DateTime data often adds complexity and dimensionality to datasets. By extracting features like year, month, and day, you reduce the number of features while retaining critical temporal information.\n",
        "\n",
        "4. **Model Compatibility**: Many machine learning algorithms require numerical input. Extracting DateTime components converts time-related information into numeric values, making it compatible with a broader range of algorithms.\n",
        "\n",
        "In essence, extracting features like month, day, and year from DateTime types is a critical step in preparing temporal data for analysis and modeling. It simplifies complex DateTime information, enhances interpretability, and empowers machine learning models to uncover meaningful insights and patterns in the data. This feature engineering process is especially valuable when dealing with time-centric datasets in various domains, including finance, healthcare, and marketing."
      ],
      "metadata": {
        "id": "IPQ1hmVWuQEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df['Start_Time'] = df['Start_Time'].astype(str)\n",
        "# df['End_Time'] = df['End_Time'].astype(str)\n",
        "# df['Weather_Timestamp']= df['Weather_Timestamp'].astype(str)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:45:21.579696Z",
          "iopub.execute_input": "2023-09-19T16:45:21.580395Z",
          "iopub.status.idle": "2023-09-19T16:45:27.950919Z",
          "shell.execute_reply.started": "2023-09-19T16:45:21.580348Z",
          "shell.execute_reply": "2023-09-19T16:45:27.949511Z"
        },
        "trusted": true,
        "id": "U8FsfwRwuQEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df2['Start_Time']= df['Start_Time']\n",
        "# df2['End_Time']=df['End_Time']\n",
        "# df2['Weather_Timestamp']=df['Weather_Timestamp']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:45:27.952364Z",
          "iopub.execute_input": "2023-09-19T16:45:27.952724Z",
          "iopub.status.idle": "2023-09-19T16:45:30.903537Z",
          "shell.execute_reply.started": "2023-09-19T16:45:27.952692Z",
          "shell.execute_reply": "2023-09-19T16:45:30.902254Z"
        },
        "trusted": true,
        "id": "QGgUuhFTuQEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Start_Time']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:45:30.904993Z",
          "iopub.execute_input": "2023-09-19T16:45:30.905345Z",
          "iopub.status.idle": "2023-09-19T16:45:30.914690Z",
          "shell.execute_reply.started": "2023-09-19T16:45:30.905313Z",
          "shell.execute_reply": "2023-09-19T16:45:30.913602Z"
        },
        "trusted": true,
        "id": "YhFEkFlbuQEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['Start_Date', 'Start_Time']] = df2['Start_Time'].str.split(' ',expand=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:45:30.916669Z",
          "iopub.execute_input": "2023-09-19T16:45:30.917123Z",
          "iopub.status.idle": "2023-09-19T16:46:00.843777Z",
          "shell.execute_reply.started": "2023-09-19T16:45:30.917082Z",
          "shell.execute_reply": "2023-09-19T16:46:00.842433Z"
        },
        "trusted": true,
        "id": "tZm9EQvBuQEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Start_Date']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:46:00.845701Z",
          "iopub.execute_input": "2023-09-19T16:46:00.846115Z",
          "iopub.status.idle": "2023-09-19T16:46:00.854465Z",
          "shell.execute_reply.started": "2023-09-19T16:46:00.846083Z",
          "shell.execute_reply": "2023-09-19T16:46:00.853691Z"
        },
        "trusted": true,
        "id": "QA_KFmVKuQEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Start_Date'] = pd.to_datetime(df2['Start_Date'], format=\"%Y-%m-%d\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:46:00.855717Z",
          "iopub.execute_input": "2023-09-19T16:46:00.856706Z",
          "iopub.status.idle": "2023-09-19T16:46:02.798143Z",
          "shell.execute_reply.started": "2023-09-19T16:46:00.856674Z",
          "shell.execute_reply": "2023-09-19T16:46:02.797195Z"
        },
        "trusted": true,
        "id": "FDibpOsguQEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Start_Time']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:46:02.799684Z",
          "iopub.execute_input": "2023-09-19T16:46:02.800365Z",
          "iopub.status.idle": "2023-09-19T16:46:02.810019Z",
          "shell.execute_reply.started": "2023-09-19T16:46:02.800318Z",
          "shell.execute_reply": "2023-09-19T16:46:02.808853Z"
        },
        "trusted": true,
        "id": "QguVzZVCuQEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Start_Time'] = pd.to_timedelta(df2['Start_Time'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:46:02.811836Z",
          "iopub.execute_input": "2023-09-19T16:46:02.812179Z",
          "iopub.status.idle": "2023-09-19T16:46:13.238438Z",
          "shell.execute_reply.started": "2023-09-19T16:46:02.812149Z",
          "shell.execute_reply": "2023-09-19T16:46:13.237128Z"
        },
        "trusted": true,
        "id": "RGPwLvRPuQEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['Weather_Date', 'Weather_Time']] = df2['Weather_Timestamp'].str.split(' ',expand=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:46:13.240034Z",
          "iopub.execute_input": "2023-09-19T16:46:13.240444Z",
          "iopub.status.idle": "2023-09-19T16:46:34.262812Z",
          "shell.execute_reply.started": "2023-09-19T16:46:13.240411Z",
          "shell.execute_reply": "2023-09-19T16:46:34.261754Z"
        },
        "trusted": true,
        "id": "nkRCKV5buQEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Weather_Date'] = pd.to_datetime(df2['Weather_Date'], format=\"%Y-%m-%d\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:46:34.264126Z",
          "iopub.execute_input": "2023-09-19T16:46:34.264468Z",
          "iopub.status.idle": "2023-09-19T16:46:36.561709Z",
          "shell.execute_reply.started": "2023-09-19T16:46:34.264438Z",
          "shell.execute_reply": "2023-09-19T16:46:36.560624Z"
        },
        "trusted": true,
        "id": "wK1SYoGduQEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Weather_Time'] = pd.to_timedelta(df2['Weather_Time'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:46:36.564259Z",
          "iopub.execute_input": "2023-09-19T16:46:36.564711Z",
          "iopub.status.idle": "2023-09-19T16:46:46.378980Z",
          "shell.execute_reply.started": "2023-09-19T16:46:36.564666Z",
          "shell.execute_reply": "2023-09-19T16:46:46.377723Z"
        },
        "trusted": true,
        "id": "hj9yqQnTuQEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['End_Date', 'End_Time']] = df2['End_Time'].str.split(' ',expand=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:46:46.380605Z",
          "iopub.execute_input": "2023-09-19T16:46:46.381092Z",
          "iopub.status.idle": "2023-09-19T16:47:07.515531Z",
          "shell.execute_reply.started": "2023-09-19T16:46:46.381048Z",
          "shell.execute_reply": "2023-09-19T16:47:07.514452Z"
        },
        "trusted": true,
        "id": "5lfm_MB6uQEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['End_Date'] = pd.to_datetime(df2['End_Date'], format=\"%Y-%m-%d\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:47:07.516871Z",
          "iopub.execute_input": "2023-09-19T16:47:07.517212Z",
          "iopub.status.idle": "2023-09-19T16:47:09.977414Z",
          "shell.execute_reply.started": "2023-09-19T16:47:07.517182Z",
          "shell.execute_reply": "2023-09-19T16:47:09.976173Z"
        },
        "trusted": true,
        "id": "cHQNLoxpuQEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['End_Time'] = pd.to_timedelta(df2['End_Time'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:47:09.979136Z",
          "iopub.execute_input": "2023-09-19T16:47:09.979787Z",
          "iopub.status.idle": "2023-09-19T16:47:20.925067Z",
          "shell.execute_reply.started": "2023-09-19T16:47:09.979749Z",
          "shell.execute_reply": "2023-09-19T16:47:20.923708Z"
        },
        "trusted": true,
        "id": "b3VZ4DIOuQEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Start_Time'] = df2['Start_Time'].astype(str).str.replace('0 days ', '')\n",
        "df2['Start_Time']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:47:20.926639Z",
          "iopub.execute_input": "2023-09-19T16:47:20.927019Z",
          "iopub.status.idle": "2023-09-19T16:49:13.477200Z",
          "shell.execute_reply.started": "2023-09-19T16:47:20.926987Z",
          "shell.execute_reply": "2023-09-19T16:49:13.476171Z"
        },
        "trusted": true,
        "id": "8nanZ5SmuQEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Start_month'] = df2['Start_Date'].dt.month\n",
        "df2['Start_day'] = df2['Start_Date'].dt.day\n",
        "df2['Start_year'] = df2['Start_Date'].dt.year"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:49:13.478565Z",
          "iopub.execute_input": "2023-09-19T16:49:13.478889Z",
          "iopub.status.idle": "2023-09-19T16:49:15.514076Z",
          "shell.execute_reply.started": "2023-09-19T16:49:13.478860Z",
          "shell.execute_reply": "2023-09-19T16:49:15.512963Z"
        },
        "trusted": true,
        "id": "0KUlpkSQuQEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['Start_month','Start_day','Start_year']].head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:49:15.515823Z",
          "iopub.execute_input": "2023-09-19T16:49:15.516199Z",
          "iopub.status.idle": "2023-09-19T16:49:18.944680Z",
          "shell.execute_reply.started": "2023-09-19T16:49:15.516167Z",
          "shell.execute_reply": "2023-09-19T16:49:18.943593Z"
        },
        "trusted": true,
        "id": "8gYnEvFXuQEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['End_month'] = df2['End_Date'].dt.month\n",
        "df2['End_day'] = df2['End_Date'].dt.day\n",
        "df2['End_year'] = df2['End_Date'].dt.year"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:49:18.946237Z",
          "iopub.execute_input": "2023-09-19T16:49:18.946670Z",
          "iopub.status.idle": "2023-09-19T16:49:20.763698Z",
          "shell.execute_reply.started": "2023-09-19T16:49:18.946640Z",
          "shell.execute_reply": "2023-09-19T16:49:20.762331Z"
        },
        "trusted": true,
        "id": "9fv1ls_ZuQEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Weather_month'] = df2['Weather_Date'].dt.month\n",
        "df2['Weather_day'] = df2['Weather_Date'].dt.day\n",
        "df2['Weather_year'] = df2['Weather_Date'].dt.year"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:49:20.764972Z",
          "iopub.execute_input": "2023-09-19T16:49:20.765282Z",
          "iopub.status.idle": "2023-09-19T16:49:22.614324Z",
          "shell.execute_reply.started": "2023-09-19T16:49:20.765254Z",
          "shell.execute_reply": "2023-09-19T16:49:22.613139Z"
        },
        "trusted": true,
        "id": "0SB4dVJZuQEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parts = df2['Start_Time'].astype(str).str.split(':')\n",
        "\n",
        "df2['Start_Hour'] = parts.str[0]\n",
        "df2['Start_Mins'] = parts.str[1]\n",
        "df2['Start_seconds'] = parts.str[2]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:49:22.615591Z",
          "iopub.execute_input": "2023-09-19T16:49:22.615934Z",
          "iopub.status.idle": "2023-09-19T16:49:59.158097Z",
          "shell.execute_reply.started": "2023-09-19T16:49:22.615882Z",
          "shell.execute_reply": "2023-09-19T16:49:59.157002Z"
        },
        "trusted": true,
        "id": "hsnuVGA2uQEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2[['Start_Hour','Start_Mins','Start_seconds']].head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:49:59.159483Z",
          "iopub.execute_input": "2023-09-19T16:49:59.159898Z",
          "iopub.status.idle": "2023-09-19T16:50:05.626365Z",
          "shell.execute_reply.started": "2023-09-19T16:49:59.159856Z",
          "shell.execute_reply": "2023-09-19T16:50:05.625143Z"
        },
        "trusted": true,
        "id": "L-pmFDz_uQEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parts = df2['End_Time'].astype(str).str.split(':')\n",
        "\n",
        "df2['End_Hour'] = parts.str[0]\n",
        "df2['End_Mins'] = parts.str[1]\n",
        "df2['End_seconds'] = parts.str[2]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:50:05.627808Z",
          "iopub.execute_input": "2023-09-19T16:50:05.628305Z",
          "iopub.status.idle": "2023-09-19T16:52:24.629514Z",
          "shell.execute_reply.started": "2023-09-19T16:50:05.628264Z",
          "shell.execute_reply": "2023-09-19T16:52:24.628182Z"
        },
        "trusted": true,
        "id": "cqVIqENYuQEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parts = df2['Weather_Time'].astype(str).str.split(':')\n",
        "\n",
        "df2['Weather_Hour'] = parts.str[0]\n",
        "df2['Weather_Mins'] = parts.str[1]\n",
        "df2['Weather_seconds'] = parts.str[2]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:52:24.631074Z",
          "iopub.execute_input": "2023-09-19T16:52:24.631440Z",
          "iopub.status.idle": "2023-09-19T16:54:45.783428Z",
          "shell.execute_reply.started": "2023-09-19T16:52:24.631407Z",
          "shell.execute_reply": "2023-09-19T16:54:45.782255Z"
        },
        "trusted": true,
        "id": "4udI9jAeuQEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/kaggle/working/df2.csv'\n",
        "df2.to_csv(file_path, index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T16:54:45.784765Z",
          "iopub.execute_input": "2023-09-19T16:54:45.785107Z",
          "iopub.status.idle": "2023-09-19T17:02:15.846952Z",
          "shell.execute_reply.started": "2023-09-19T16:54:45.785079Z",
          "shell.execute_reply": "2023-09-19T17:02:15.845235Z"
        },
        "trusted": true,
        "id": "a-4cTyH1uQEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating New Features"
      ],
      "metadata": {
        "id": "g-0m4pBOuQEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The motivation behind creating new features is multifaceted and plays a pivotal role in enhancing the quality and informativeness of the data used for modeling. Here are some key reasons why we create new features in feature engineering:\n",
        "\n",
        "1. **Improved Model Performance**: New features can capture previously hidden patterns and relationships within the data, leading to better model accuracy and predictive power. These engineered features provide the model with more discriminative information to make informed decisions.\n",
        "\n",
        "2. **Handling Complex Relationships**: In real-world datasets, complex relationships between variables often exist. By creating new features that encode these relationships, we can simplify the modeling process and enable the model to capture intricate dependencies.\n",
        "\n",
        "3. **Dimensionality Reduction**: Carefully engineered features can help reduce the dimensionality of the dataset by summarizing information in a more compact form. This can lead to faster training times and reduced risk of overfitting, especially when dealing with high-dimensional data.\n",
        "\n",
        "In summary, the creation of new features is a crucial step in the data preprocessing pipeline. It enables data scientists and machine learning practitioners to extract relevant information, address data challenges, and empower models to make accurate predictions or classifications. Effective feature engineering can be the difference between a mediocre model and one that excels in solving real-world problems across various domains."
      ],
      "metadata": {
        "id": "BAf_8d7FuQEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df2= pd.read_csv('/kaggle/working/df2.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:15.849952Z",
          "iopub.execute_input": "2023-09-19T17:02:15.850689Z",
          "iopub.status.idle": "2023-09-19T17:02:15.855299Z",
          "shell.execute_reply.started": "2023-09-19T17:02:15.850647Z",
          "shell.execute_reply": "2023-09-19T17:02:15.854381Z"
        },
        "trusted": true,
        "id": "9jytjnMTuQEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.dtypes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:15.857107Z",
          "iopub.execute_input": "2023-09-19T17:02:15.857706Z",
          "iopub.status.idle": "2023-09-19T17:02:15.879490Z",
          "shell.execute_reply.started": "2023-09-19T17:02:15.857673Z",
          "shell.execute_reply": "2023-09-19T17:02:15.878349Z"
        },
        "trusted": true,
        "id": "C81hPGHKuQEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Comfort_Index'] = (df2['Temperature(F)'] - 32) * (df2['Humidity(%)'] / 100)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:15.894531Z",
          "iopub.execute_input": "2023-09-19T17:02:15.895005Z",
          "iopub.status.idle": "2023-09-19T17:02:15.960264Z",
          "shell.execute_reply.started": "2023-09-19T17:02:15.894943Z",
          "shell.execute_reply": "2023-09-19T17:02:15.959288Z"
        },
        "trusted": true,
        "id": "cRiKpn9UuQEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Comfort_Index'].head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:15.962838Z",
          "iopub.execute_input": "2023-09-19T17:02:15.963183Z",
          "iopub.status.idle": "2023-09-19T17:02:15.972184Z",
          "shell.execute_reply.started": "2023-09-19T17:02:15.963155Z",
          "shell.execute_reply": "2023-09-19T17:02:15.970953Z"
        },
        "trusted": true,
        "id": "H-p_MxNLuQEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.drop(columns= ['Temperature(F)','Humidity(%)','Wind_Speed(mph)'],inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:15.973649Z",
          "iopub.execute_input": "2023-09-19T17:02:15.974005Z",
          "iopub.status.idle": "2023-09-19T17:02:26.720719Z",
          "shell.execute_reply.started": "2023-09-19T17:02:15.973976Z",
          "shell.execute_reply": "2023-09-19T17:02:26.719604Z"
        },
        "trusted": true,
        "id": "nv5lNN1XuQEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['End_Time'] = pd.to_datetime(df['End_Time'])\n",
        "df['Start_Time'] = pd.to_datetime(df['Start_Time'])\n",
        "df2['Accident_Duration'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60.0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:26.722304Z",
          "iopub.execute_input": "2023-09-19T17:02:26.722773Z",
          "iopub.status.idle": "2023-09-19T17:02:31.611298Z",
          "shell.execute_reply.started": "2023-09-19T17:02:26.722732Z",
          "shell.execute_reply": "2023-09-19T17:02:31.610058Z"
        },
        "trusted": true,
        "id": "5BlwUWCMuQEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Accident_Duration'].head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:31.612839Z",
          "iopub.execute_input": "2023-09-19T17:02:31.613199Z",
          "iopub.status.idle": "2023-09-19T17:02:31.623029Z",
          "shell.execute_reply.started": "2023-09-19T17:02:31.613166Z",
          "shell.execute_reply": "2023-09-19T17:02:31.621832Z"
        },
        "trusted": true,
        "id": "BiNSac3guQEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Number_Accidents_Street = df.groupby(['Street'])['ID'].count()\n",
        "Number_Accidents_Street = Number_Accidents_Street.sort_values(ascending = False)\n",
        "Number_Accidents_Street"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:31.624938Z",
          "iopub.execute_input": "2023-09-19T17:02:31.625741Z",
          "iopub.status.idle": "2023-09-19T17:02:34.555195Z",
          "shell.execute_reply.started": "2023-09-19T17:02:31.625695Z",
          "shell.execute_reply": "2023-09-19T17:02:34.554085Z"
        },
        "trusted": true,
        "id": "wXNoMY1guQEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U googlemaps"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:34.556745Z",
          "iopub.execute_input": "2023-09-19T17:02:34.557217Z",
          "iopub.status.idle": "2023-09-19T17:02:55.238229Z",
          "shell.execute_reply.started": "2023-09-19T17:02:34.557172Z",
          "shell.execute_reply": "2023-09-19T17:02:55.236899Z"
        },
        "trusted": true,
        "id": "tWFSiA2cuQEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def geocode_street(address, gmaps):\n",
        "    geocode_result = gmaps.geocode(address)\n",
        "    if not geocode_result.empty:\n",
        "        location = geocode_result.iloc[0]['geometry']['location']\n",
        "        return location['lat'], location['lng']\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def calculate_street_area(api_key, street_address):\n",
        "    # Initialize the Google Maps client\n",
        "    gmaps = googlemaps.Client(key=api_key)\n",
        "\n",
        "    # Get the coordinates of the street\n",
        "    street_coordinates = geocode_street(street_address, gmaps)\n",
        "\n",
        "    if street_coordinates:\n",
        "        # Create a polygon representing the street (adjust the polygon as needed)\n",
        "        polygon = Polygon([\n",
        "            (street_coordinates[1] - 0.0001, street_coordinates[0] - 0.0001),\n",
        "            (street_coordinates[1] + 0.0001, street_coordinates[0] - 0.0001),\n",
        "            (street_coordinates[1] + 0.0001, street_coordinates[0] + 0.0001),\n",
        "            (street_coordinates[1] - 0.0001, street_coordinates[0] + 0.0001),\n",
        "        ])\n",
        "\n",
        "        # Create a GeoDataFrame from the polygon\n",
        "        gdf = gpd.GeoDataFrame(geometry=[polygon], crs='EPSG:4326')\n",
        "\n",
        "        # Calculate the area of the street polygon\n",
        "        street_area = gdf.to_crs('EPSG:3857').area.values[0]  # Area in square meters\n",
        "\n",
        "        return street_area\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:55.240956Z",
          "iopub.execute_input": "2023-09-19T17:02:55.241514Z",
          "iopub.status.idle": "2023-09-19T17:02:55.256177Z",
          "shell.execute_reply.started": "2023-09-19T17:02:55.241458Z",
          "shell.execute_reply": "2023-09-19T17:02:55.254773Z"
        },
        "trusted": true,
        "id": "MQpLHwo2uQEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df2['Accident_density_by_Street'] = Number_Accidents_Street/ calculate_street_area('AIzaSyDOLO0844kOC--yuvVkLTee1u7qhdLALwQ',df2['Street'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:55.257888Z",
          "iopub.execute_input": "2023-09-19T17:02:55.258314Z",
          "iopub.status.idle": "2023-09-19T17:02:55.275676Z",
          "shell.execute_reply.started": "2023-09-19T17:02:55.258282Z",
          "shell.execute_reply": "2023-09-19T17:02:55.274554Z"
        },
        "trusted": true,
        "id": "AoV-6hSvuQEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_christmas(start_time):\n",
        "    try:\n",
        "        return start_time.month == 12 and start_time.day == 25\n",
        "    except ValueError:\n",
        "        print(\"error\")\n",
        "        return False\n",
        "\n",
        "df2['Is_Christmas'] = df2['Start_Date'].apply(lambda x: 'Yes' if is_christmas(x) else 'No')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:02:55.277317Z",
          "iopub.execute_input": "2023-09-19T17:02:55.278376Z",
          "iopub.status.idle": "2023-09-19T17:03:16.614034Z",
          "shell.execute_reply.started": "2023-09-19T17:02:55.278337Z",
          "shell.execute_reply": "2023-09-19T17:03:16.612675Z"
        },
        "trusted": true,
        "id": "8HUjB3pQuQEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_rush_hour(start_time):\n",
        "    parts = start_time.split(':')\n",
        "    hour = parts[0]\n",
        "\n",
        "    try:\n",
        "        if 6 <= int(hour) < 10:  # Morning rush hour from 6 AM to 10 AM\n",
        "            return 'Yes'\n",
        "        elif 16 <= int(hour) < 19:  # Evening rush hour from 4 PM to 7 PM\n",
        "            return 'Yes'\n",
        "        else:\n",
        "            return 'No'\n",
        "    except ValueError:\n",
        "        return 'Invalid'\n",
        "\n",
        "df2['Is_Rush_Hour'] = df2['Start_Time'].apply(categorize_rush_hour)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:03:16.615787Z",
          "iopub.execute_input": "2023-09-19T17:03:16.616172Z",
          "iopub.status.idle": "2023-09-19T17:03:23.348108Z",
          "shell.execute_reply.started": "2023-09-19T17:03:16.616140Z",
          "shell.execute_reply": "2023-09-19T17:03:23.346959Z"
        },
        "trusted": true,
        "id": "U7aSU9AduQEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Is_Rush_Hour']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:03:23.349793Z",
          "iopub.execute_input": "2023-09-19T17:03:23.350175Z",
          "iopub.status.idle": "2023-09-19T17:03:23.360860Z",
          "shell.execute_reply.started": "2023-09-19T17:03:23.350143Z",
          "shell.execute_reply": "2023-09-19T17:03:23.359669Z"
        },
        "trusted": true,
        "id": "dKatikLcuQEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Radius of the Earth in kilometers\n",
        "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
        "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:03:23.362707Z",
          "iopub.execute_input": "2023-09-19T17:03:23.363468Z",
          "iopub.status.idle": "2023-09-19T17:03:23.374610Z",
          "shell.execute_reply.started": "2023-09-19T17:03:23.363387Z",
          "shell.execute_reply": "2023-09-19T17:03:23.373376Z"
        },
        "trusted": true,
        "id": "QE5UGvbIuQEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_distance(start_lng, start_lat):\n",
        "# Replace with your Google Places API key\n",
        "    api_key = \"AIzaSyDOLO0844kOC--yuvVkLTee1u7qhdLALwQ\"\n",
        "\n",
        "    start_lat = start_lng\n",
        "    start_lng = start_lat\n",
        "\n",
        "    place_type = \"hospital\"\n",
        "\n",
        "    base_url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
        "    params = {\n",
        "        \"location\": f\"{start_lat},{start_lng}\",\n",
        "        \"radius\": 5000,\n",
        "        \"type\": place_type,\n",
        "        \"key\": api_key,\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    if data.get(\"results\"):\n",
        "        nearest_hospital = data[\"results\"][0]\n",
        "        hospital_name = nearest_hospital[\"name\"]\n",
        "        hospital_lat = nearest_hospital[\"geometry\"][\"location\"][\"lat\"]\n",
        "        hospital_lng = nearest_hospital[\"geometry\"][\"location\"][\"lng\"]\n",
        "\n",
        "        distance_to_hospital = haversine(start_lat, start_lng, hospital_lat, hospital_lng)\n",
        "\n",
        "        return (f\"The nearest hospital is {hospital_name}.\")\n",
        "#         print(f\"The distance to the nearest hospital is approximately {distance_to_hospital:.2f} km.\")\n",
        "#     else:\n",
        "#         print(\"No hospitals found nearby.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:03:23.377094Z",
          "iopub.execute_input": "2023-09-19T17:03:23.377454Z",
          "iopub.status.idle": "2023-09-19T17:03:23.388751Z",
          "shell.execute_reply.started": "2023-09-19T17:03:23.377418Z",
          "shell.execute_reply": "2023-09-19T17:03:23.387567Z"
        },
        "trusted": true,
        "id": "Qx3FnX-0uQEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sampled_df['Distance_from_nearest_hospital']= get_distance(sampled_df['Start_Lat'],sampled_df['Start_Lng'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:03:23.390375Z",
          "iopub.execute_input": "2023-09-19T17:03:23.390968Z",
          "iopub.status.idle": "2023-09-19T17:03:23.402071Z",
          "shell.execute_reply.started": "2023-09-19T17:03:23.390903Z",
          "shell.execute_reply": "2023-09-19T17:03:23.400935Z"
        },
        "trusted": true,
        "id": "kDZW9NjsuQEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.drop(columns=['End_Time','Start_Time','Weather_Time','Weather_Timestamp','Start_Date','End_Date','Weather_Date'],inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:03:23.403054Z",
          "iopub.execute_input": "2023-09-19T17:03:23.403362Z",
          "iopub.status.idle": "2023-09-19T17:03:28.584001Z",
          "shell.execute_reply.started": "2023-09-19T17:03:23.403336Z",
          "shell.execute_reply": "2023-09-19T17:03:28.582664Z"
        },
        "trusted": true,
        "id": "LHgKMMqIuQEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_dff = df2.sample(frac=0.1, random_state=42)\n",
        "sampled_dff.dropna(inplace=True)\n",
        "categorical_cols = sampled_dff.select_dtypes(include=['object','bool','category']).columns\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in categorical_cols:\n",
        "    sampled_dff[col] = label_encoder.fit_transform(sampled_dff[col])\n",
        "\n",
        "X = sampled_dff.drop('Severity', axis=1)\n",
        "y= sampled_dff['Severity']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# print(classification_task(rf_classifier,X_train, y_train ,X_test,y_test, y_pred,'rf'))\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:03:28.585541Z",
          "iopub.execute_input": "2023-09-19T17:03:28.585864Z",
          "iopub.status.idle": "2023-09-19T17:07:27.865100Z",
          "shell.execute_reply.started": "2023-09-19T17:03:28.585837Z",
          "shell.execute_reply": "2023-09-19T17:07:27.863616Z"
        },
        "trusted": true,
        "id": "njAMY8GUuQEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Encoding"
      ],
      "metadata": {
        "id": "rMCFZn-puQEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label encoding is a common technique in feature engineering that transforms categorical data into numerical values. It assigns a unique integer to each category or label within a categorical feature, thereby converting it into a format that machine learning algorithms can readily understand and process. Label encoding is used for several important reasons in feature engineering:\n",
        "\n",
        "1. **Numerical Representation**: Many machine learning algorithms, especially those based on mathematical equations, require numerical input. Label encoding enables the representation of categorical data in a numeric form, making it compatible with such algorithms.\n",
        "\n",
        "2. **Simplification of Data**: Label encoding simplifies the categorical data by reducing it to a set of integers. This simplification makes the data more manageable and easier for models to work with.\n",
        "\n",
        "3. **Memory Efficiency**: Numeric representation typically requires less memory compared to storing categorical strings, which can be especially advantageous when working with large datasets.\n",
        "\n",
        "4. **Preserving Ordinal Information**: Label encoding is suitable for ordinal categorical data, where the order of categories holds meaning. By assigning integers in a specific order, label encoding preserves this ordinal information.\n",
        "\n",
        "5. **Improved Model Performance**: Some machine learning algorithms may perform better on label-encoded categorical features, especially when there is a clear ordinal relationship or when the number of categories is relatively small.\n",
        "\n",
        "7. **Handling Categorical Features in Tree-Based Models**: Tree-based models like decision trees and random forests can naturally handle label-encoded features without the need for one-hot encoding.\n",
        "\n",
        "In summary, label encoding is a valuable technique in feature engineering that transforms categorical data into a numeric format, making it suitable for a wide range of machine learning algorithms. It simplifies data, preserves ordinal information, and can improve model performance in certain situations. Data scientists should carefully consider the nature of the categorical data and the requirements of the chosen machine learning algorithm when deciding whether to use label encoding or other encoding methods."
      ],
      "metadata": {
        "id": "TJmACw9ruQEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2['End_Hour'] = df2['End_Hour'].str.replace('0 days', '')\n",
        "df2['Weather_Hour'] = df2['Weather_Hour'].str.replace('0 days', '')\n",
        "df2['Start_Hour'] = df2['Start_Hour'].str.replace('0 days', '')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:12:19.800779Z",
          "iopub.execute_input": "2023-09-19T17:12:19.801577Z",
          "iopub.status.idle": "2023-09-19T17:12:44.322858Z",
          "shell.execute_reply.started": "2023-09-19T17:12:19.801518Z",
          "shell.execute_reply": "2023-09-19T17:12:44.321764Z"
        },
        "trusted": true,
        "id": "H8NQqe1BuQEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_convert = [\n",
        "    'Start_Mins', 'Weather_Hour', 'Weather_Mins', 'End_seconds',\n",
        "    'Weather_seconds', 'End_Mins', 'End_Hour', 'Start_seconds', 'Start_Hour'\n",
        "]\n",
        "\n",
        "for column in columns_to_convert:\n",
        "    df2[column] = df2[column].astype(int)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:12:46.534923Z",
          "iopub.execute_input": "2023-09-19T17:12:46.535503Z",
          "iopub.status.idle": "2023-09-19T17:13:01.752945Z",
          "shell.execute_reply.started": "2023-09-19T17:12:46.535459Z",
          "shell.execute_reply": "2023-09-19T17:13:01.751275Z"
        },
        "trusted": true,
        "id": "RnkQ02Z5uQE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = df2.select_dtypes(include=['object','bool','category']).columns\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in categorical_cols:\n",
        "    df2[col] = label_encoder.fit_transform(df2[col])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:13:01.755165Z",
          "iopub.execute_input": "2023-09-19T17:13:01.755600Z",
          "iopub.status.idle": "2023-09-19T17:13:40.924109Z",
          "shell.execute_reply.started": "2023-09-19T17:13:01.755563Z",
          "shell.execute_reply": "2023-09-19T17:13:40.922870Z"
        },
        "trusted": true,
        "id": "9e8EZW8MuQE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/kaggle/working/df3.csv'\n",
        "df2.to_csv(file_path, index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:53:36.515709Z",
          "iopub.execute_input": "2023-09-19T17:53:36.516076Z",
          "iopub.status.idle": "2023-09-19T17:53:36.545838Z",
          "shell.execute_reply.started": "2023-09-19T17:53:36.516048Z",
          "shell.execute_reply": "2023-09-19T17:53:36.544396Z"
        },
        "trusted": true,
        "id": "MQo7MuWOuQE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2= pd.read_csv('/kaggle/working/df3.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:54:18.872483Z",
          "iopub.execute_input": "2023-09-19T17:54:18.873223Z",
          "iopub.status.idle": "2023-09-19T17:54:19.633677Z",
          "shell.execute_reply.started": "2023-09-19T17:54:18.873182Z",
          "shell.execute_reply": "2023-09-19T17:54:19.631716Z"
        },
        "trusted": true,
        "id": "xTuAxA3wuQE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:18:14.122689Z",
          "iopub.execute_input": "2023-09-19T17:18:14.123264Z",
          "iopub.status.idle": "2023-09-19T17:18:14.166999Z",
          "shell.execute_reply.started": "2023-09-19T17:18:14.123217Z",
          "shell.execute_reply": "2023-09-19T17:18:14.165623Z"
        },
        "trusted": true,
        "id": "tbanKEv5uQE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling the Dataset Randomly (due its large size)"
      ],
      "metadata": {
        "id": "9tQxgCLBuQE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.dropna(inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:18:08.958461Z",
          "iopub.execute_input": "2023-09-19T17:18:08.958822Z",
          "iopub.status.idle": "2023-09-19T17:18:10.155761Z",
          "shell.execute_reply.started": "2023-09-19T17:18:08.958791Z",
          "shell.execute_reply": "2023-09-19T17:18:10.154626Z"
        },
        "trusted": true,
        "id": "Nq51p_1NuQE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = df2.sample(frac=0.1, random_state=42)\n",
        "sampled_df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:18:10.157521Z",
          "iopub.execute_input": "2023-09-19T17:18:10.158564Z",
          "iopub.status.idle": "2023-09-19T17:18:11.037594Z",
          "shell.execute_reply.started": "2023-09-19T17:18:10.158519Z",
          "shell.execute_reply": "2023-09-19T17:18:11.036424Z"
        },
        "trusted": true,
        "id": "FptRkDc8uQE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection"
      ],
      "metadata": {
        "id": "PQtXU5sduQE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Matrix with Heatmap\n",
        "\n",
        "\n",
        "A correlation matrix is a fundamental tool in feature engineering and data analysis that helps us understand the relationships between variables in a dataset. It provides a matrix of correlation coefficients, which quantify the degree and direction of linear associations between pairs of features. The correlation matrix is valuable for several key reasons when working with data:\n",
        "\n",
        "1. **Identifying Relationships**: It enables data scientists to quickly identify whether and how variables are related to each other. Positive correlations suggest that when one variable increases, the other tends to increase as well, while negative correlations indicate an inverse relationship.\n",
        "\n",
        "2. **Feature Selection**: In feature engineering, a correlation matrix can be used for feature selection by identifying features that are highly correlated with the target variable. These features are likely to have a strong influence on the target and can be prioritized for modeling.\n",
        "\n",
        "3. **Multicollinearity Detection**: High correlations between predictor variables (independent features) can indicate multicollinearity, where two or more features are redundant and convey similar information. Detecting multicollinearity is essential for regression models to avoid issues like unstable coefficient estimates.\n"
      ],
      "metadata": {
        "id": "pykdbeBGuQE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:11:17.418879Z",
          "iopub.execute_input": "2023-09-19T15:11:17.419335Z",
          "iopub.status.idle": "2023-09-19T15:11:17.428033Z",
          "shell.execute_reply.started": "2023-09-19T15:11:17.419297Z",
          "shell.execute_reply": "2023-09-19T15:11:17.426765Z"
        },
        "trusted": true,
        "id": "mB2ZefW5uQE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_numerical = df.select_dtypes(include=['int64','float64'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:23:55.970813Z",
          "iopub.execute_input": "2023-09-19T17:23:55.971301Z",
          "iopub.status.idle": "2023-09-19T17:23:56.181967Z",
          "shell.execute_reply.started": "2023-09-19T17:23:55.971256Z",
          "shell.execute_reply": "2023-09-19T17:23:56.180520Z"
        },
        "trusted": true,
        "id": "dFlzFPGsuQE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(19, 8))\n",
        "sns.set(style=\"white\")\n",
        "mask = np.triu(df_numerical.corr())\n",
        "sns.heatmap(data=df_numerical.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', mask=mask)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:23:56.183372Z",
          "iopub.execute_input": "2023-09-19T17:23:56.183713Z",
          "iopub.status.idle": "2023-09-19T17:24:01.908792Z",
          "shell.execute_reply.started": "2023-09-19T17:23:56.183685Z",
          "shell.execute_reply": "2023-09-19T17:24:01.907594Z"
        },
        "trusted": true,
        "id": "4JwW7SCnuQE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df_1 = df.sample(frac=0.1, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T15:11:24.681211Z",
          "iopub.execute_input": "2023-09-19T15:11:24.681776Z",
          "iopub.status.idle": "2023-09-19T15:11:31.198558Z",
          "shell.execute_reply.started": "2023-09-19T15:11:24.681729Z",
          "shell.execute_reply": "2023-09-19T15:11:31.197249Z"
        },
        "trusted": true,
        "id": "0hLNEL2MuQE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VIF Test\n",
        "\n",
        "The Variance Inflation Factor (VIF) test is a statistical technique commonly used in feature engineering and model building. Its primary purpose is to assess multicollinearity, which is the presence of high correlations between predictor variables (features) in a regression model. Multicollinearity can lead to unstable coefficient estimates and reduce the interpretability of model results. The VIF test helps identify and mitigate multicollinearity by quantifying the degree to which the variance of an estimated regression coefficient is inflated due to correlations with other predictors.\n",
        "\n",
        "Key Reasons for Using the VIF Test in Feature Engineering:\n",
        "\n",
        "1. **Multicollinearity Detection**: The VIF test is a powerful tool for identifying multicollinearity among predictor variables. It quantifies the extent to which the variability of one predictor can be explained by other predictors in the model.\n",
        "\n",
        "2. **Feature Selection**: When dealing with multicollinearity, the VIF test can guide feature selection decisions. Variables with high VIF scores (indicating high multicollinearity) may be candidates for removal from the model to improve its performance.\n",
        "\n",
        "3. **Avoiding Misleading Results**: Without addressing multicollinearity, it's possible to obtain coefficient estimates that are counterintuitive or misleading. The VIF test helps ensure that the relationships between predictors and the target variable are accurately captured.\n",
        "\n",
        "In summary, the VIF test is a crucial tool in feature engineering and model building that helps detect and address multicollinearity among predictor variables. By reducing multicollinearity, data scientists can improve the stability and interpretability of their models, enhance feature selection decisions, and ensure that their models meet the assumptions required for valid statistical inference. It is particularly valuable when working with regression models and other techniques that involve linear relationships between predictors and the target variable."
      ],
      "metadata": {
        "id": "zF79xbq9uQE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df.drop(columns = ['End_year','End_month','End_Mins','End_day','Start_year','Start_month','Start_day','Start_Mins','Start_seconds','Start_Hour','End_Hour','Weather_day','Weather_month','Weather_Mins','Weather_Hour','Weather_seconds'],inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "F4gE5BPKuQE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = sampled_df.drop('Severity', axis=1)\n",
        "y= sampled_df['Severity']\n",
        "X.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:36:08.850122Z",
          "iopub.execute_input": "2023-09-19T17:36:08.850689Z",
          "iopub.status.idle": "2023-09-19T17:36:08.945227Z",
          "shell.execute_reply.started": "2023-09-19T17:36:08.850643Z",
          "shell.execute_reply": "2023-09-19T17:36:08.943731Z"
        },
        "trusted": true,
        "id": "fHVjw4-luQE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.dtypes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:25:19.178087Z",
          "iopub.execute_input": "2023-09-19T17:25:19.178801Z",
          "iopub.status.idle": "2023-09-19T17:25:19.191324Z",
          "shell.execute_reply.started": "2023-09-19T17:25:19.178746Z",
          "shell.execute_reply": "2023-09-19T17:25:19.189815Z"
        },
        "trusted": true,
        "id": "447VHi5LuQE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif_data = pd.DataFrame()\n",
        "X_numerical = X.select_dtypes(include=['int64','float64'])\n",
        "vif_data[\"Severity\"] = X_numerical.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_numerical.values, i) for i in range(X_numerical.shape[1])]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:36:11.459415Z",
          "iopub.execute_input": "2023-09-19T17:36:11.460311Z",
          "iopub.status.idle": "2023-09-19T17:38:47.376343Z",
          "shell.execute_reply.started": "2023-09-19T17:36:11.460272Z",
          "shell.execute_reply": "2023-09-19T17:38:47.374273Z"
        },
        "trusted": true,
        "id": "vL4vZHRUuQE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif_data.sort_values(by='VIF',ascending= False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:38:47.379296Z",
          "iopub.execute_input": "2023-09-19T17:38:47.380222Z",
          "iopub.status.idle": "2023-09-19T17:38:47.410212Z",
          "shell.execute_reply.started": "2023-09-19T17:38:47.380170Z",
          "shell.execute_reply": "2023-09-19T17:38:47.408979Z"
        },
        "trusted": true,
        "id": "q41C3aJRuQFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate Selection using SelectKBest with Chi squared"
      ],
      "metadata": {
        "id": "wtl64GAduQFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SelectKBest with the Chi-Square (χ²) test is a feature selection technique commonly employed in feature engineering and data analysis. This method is particularly useful when dealing with categorical target variables and categorical or discrete predictor variables. It helps identify which features have a statistically significant relationship with the target variable by measuring the independence or association between them.\n",
        "\n",
        "1. **Chi-Square (χ²) Test**: The Chi-Square test is a statistical test used to determine if there is a significant association between two categorical variables. In feature engineering, it assesses the relationship between each categorical predictor feature and the categorical target variable.\n",
        "\n",
        "2. **SelectKBest**: SelectKBest is a feature selection technique that ranks features based on their statistical significance. When combined with the Chi-Square test, it selects the top K features that exhibit the strongest association with the target variable.\n",
        "\n",
        "In summary, the Chi-Square (χ²) test, when used in conjunction with SelectKBest, is a valuable tool in feature engineering. It helps select the most relevant categorical features for modeling, reduces dimensionality, enhances model performance, and ensures that the model complies with assumptions regarding variable independence. This technique is particularly well-suited for datasets with categorical or discrete features and categorical target variables."
      ],
      "metadata": {
        "id": "UiJ4cF0DuQFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_columns = X.columns[(X < 0).any()]\n",
        "\n",
        "print(\"Columns with negative values:\", list(negative_columns))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:39:45.777733Z",
          "iopub.execute_input": "2023-09-19T17:39:45.779430Z",
          "iopub.status.idle": "2023-09-19T17:39:45.814472Z",
          "shell.execute_reply.started": "2023-09-19T17:39:45.779348Z",
          "shell.execute_reply": "2023-09-19T17:39:45.813408Z"
        },
        "trusted": true,
        "id": "PaSe-VnXuQFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bestfeatures = SelectKBest(score_func= chi2, k=10)\n",
        "fit = bestfeatures.fit(abs(X),y)\n",
        "\n",
        "dfscores= pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(X.columns)\n",
        "\n",
        "featureScores= pd.concat([dfcolumns,dfscores],axis=1)\n",
        "featureScores.columns=['Features','Score']\n",
        "featureScores= featureScores.sort_values(by='Score', ascending=False)\n",
        "featureScores"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:39:46.797442Z",
          "iopub.execute_input": "2023-09-19T17:39:46.798435Z",
          "iopub.status.idle": "2023-09-19T17:39:47.356302Z",
          "shell.execute_reply.started": "2023-09-19T17:39:46.798383Z",
          "shell.execute_reply": "2023-09-19T17:39:47.354823Z"
        },
        "trusted": true,
        "id": "U0c0LxAauQFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.barh(featureScores['Features'], featureScores['Score'], color='skyblue')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:39:48.607024Z",
          "iopub.execute_input": "2023-09-19T17:39:48.607635Z",
          "iopub.status.idle": "2023-09-19T17:39:49.460840Z",
          "shell.execute_reply.started": "2023-09-19T17:39:48.607582Z",
          "shell.execute_reply": "2023-09-19T17:39:49.459965Z"
        },
        "trusted": true,
        "id": "oZvdI3EouQFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featureScores= featureScores.sort_values(by='Score', ascending=True)\n",
        "featureScores"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:39:58.547312Z",
          "iopub.execute_input": "2023-09-19T17:39:58.548330Z",
          "iopub.status.idle": "2023-09-19T17:39:58.569427Z",
          "shell.execute_reply.started": "2023-09-19T17:39:58.548274Z",
          "shell.execute_reply": "2023-09-19T17:39:58.567993Z"
        },
        "trusted": true,
        "id": "PZk6QdeouQFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ExtraTreesClassifier method\n",
        "\n",
        "The Extra Trees Classifier is a powerful machine learning algorithm used for both classification tasks and feature selection. It belongs to the ensemble learning family, which combines the predictions of multiple models to improve overall performance. Extra Trees, short for Extremely Randomized Trees, is a variation of the more well-known Random Forest algorithm. It is particularly valuable for feature engineering when you need to determine the importance of different features in your dataset.\n",
        "\n",
        "Key Points about Extra Trees Classifier for Feature Importance Selection:\n",
        "\n",
        "1. **Ensemble Learning**: Extra Trees is an ensemble learning algorithm that builds multiple decision trees and combines their predictions to make more accurate and robust classifications.\n",
        "\n",
        "2. **Randomization**: What sets Extra Trees apart is its higher level of randomness during the tree-building process compared to standard Random Forests. Extra Trees randomly selects feature subsets and thresholds at each node of the tree, resulting in a more diverse set of trees.\n",
        "\n",
        "3. **Bootstrap Aggregating**: Like Random Forests, Extra Trees also uses the bootstrap aggregating (bagging) technique. It creates multiple training datasets by resampling with replacement from the original data.\n",
        "\n",
        "4. **Feature Importance**: One of the significant advantages of Extra Trees is its ability to assess the importance of individual features in making predictions. It computes a feature importance score for each variable, which quantifies its contribution to the model's performance.\n",
        "\n",
        "In conclusion, the Extra Trees Classifier is a versatile algorithm in feature engineering that not only performs classification tasks effectively but also provides valuable insights into feature importance. It helps streamline datasets, improve model performance, and enhance model interpretability by identifying the most influential features. This makes it a valuable tool for data scientists and machine learning practitioners when building predictive models."
      ],
      "metadata": {
        "id": "X3EyzelSuQFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model= ExtraTreesClassifier()\n",
        "model.fit(X,y)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:40:51.020260Z",
          "iopub.execute_input": "2023-09-19T17:40:51.021987Z",
          "iopub.status.idle": "2023-09-19T17:44:40.243366Z",
          "shell.execute_reply.started": "2023-09-19T17:40:51.021918Z",
          "shell.execute_reply": "2023-09-19T17:44:40.242396Z"
        },
        "trusted": true,
        "id": "BsFx50MsuQFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.feature_importances_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:44:40.245447Z",
          "iopub.execute_input": "2023-09-19T17:44:40.245785Z",
          "iopub.status.idle": "2023-09-19T17:44:40.829432Z",
          "shell.execute_reply.started": "2023-09-19T17:44:40.245755Z",
          "shell.execute_reply": "2023-09-19T17:44:40.828214Z"
        },
        "trusted": true,
        "id": "2H80BvDjuQFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_importances = pd.Series(model.feature_importances_, index= X.columns)\n",
        "feat_importances.nlargest(20).plot(kind='barh')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:44:40.830863Z",
          "iopub.execute_input": "2023-09-19T17:44:40.831297Z",
          "iopub.status.idle": "2023-09-19T17:44:41.814298Z",
          "shell.execute_reply.started": "2023-09-19T17:44:40.831267Z",
          "shell.execute_reply": "2023-09-19T17:44:41.812986Z"
        },
        "trusted": true,
        "id": "3avrfB-2uQFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_importances = pd.Series(model.feature_importances_, index= X.columns)\n",
        "feat_importances.nsmallest(20).plot(kind='barh')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:44:41.816821Z",
          "iopub.execute_input": "2023-09-19T17:44:41.817226Z",
          "iopub.status.idle": "2023-09-19T17:44:42.813297Z",
          "shell.execute_reply.started": "2023-09-19T17:44:41.817194Z",
          "shell.execute_reply": "2023-09-19T17:44:42.812008Z"
        },
        "trusted": true,
        "id": "NiEH_Q82uQFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Method\n",
        "\n",
        "The Random Forest Classifier is a popular ensemble learning algorithm used for both classification and regression tasks in machine learning. It is renowned for its robustness, accuracy, and capability to assess feature importance. Random Forest builds multiple decision trees and aggregates their predictions to make more reliable and interpretable predictions.\n",
        "\n",
        "Key Points about Random Forest Classifier for Feature Importance Selection:\n",
        "\n",
        "1. **Ensemble Learning**: Random Forest is an ensemble learning method that combines the predictions of multiple decision trees. This ensemble approach reduces overfitting and increases the stability and accuracy of predictions.\n",
        "\n",
        "2. **Bootstrap Aggregating (Bagging)**: Random Forest employs bagging, a technique that creates multiple bootstrap samples (randomly resampled subsets with replacement) from the training data. Each decision tree in the forest is trained on one of these samples.\n",
        "\n",
        "3. **Feature Randomization**: To assess feature importance, Random Forest introduces additional randomization during the tree-building process. It randomly selects a subset of features at each node split, leading to diverse trees.\n",
        "\n",
        "4. **Voting or Averaging**: In classification tasks, Random Forest combines the predictions of individual trees by either majority voting (for classification) or averaging (for regression). This ensemble strategy helps improve model generalization.\n",
        "\n",
        "5. **Feature Selection**: Random Forest is widely used to select important features from a dataset. It assigns an importance score to each feature based on its contribution to reducing impurity or error in the decision trees. Features with higher importance scores are considered more influential in making predictions.\n",
        "\n",
        "\n",
        "In summary, the Random Forest Classifier is a versatile and widely used algorithm in feature engineering, not only for its strong predictive capabilities but also for its ability to assess feature importance. It aids in selecting the most influential features, reducing dimensionality, enhancing model interpretability, and improving model performance. This makes it a valuable tool for data scientists and machine learning practitioners in various domains."
      ],
      "metadata": {
        "id": "HI3iVdiouQFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "rf_classifier = RandomForestClassifier()\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "rf_classifier.feature_importances_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:44:42.814570Z",
          "iopub.execute_input": "2023-09-19T17:44:42.814952Z",
          "iopub.status.idle": "2023-09-19T17:47:19.877520Z",
          "shell.execute_reply.started": "2023-09-19T17:44:42.814892Z",
          "shell.execute_reply": "2023-09-19T17:47:19.876171Z"
        },
        "trusted": true,
        "id": "qbIVVrGXuQFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_importances = pd.Series(rf_classifier.feature_importances_, index= X.columns)\n",
        "feat_importances.nlargest(20).plot(kind='barh')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:47:19.879411Z",
          "iopub.execute_input": "2023-09-19T17:47:19.879875Z",
          "iopub.status.idle": "2023-09-19T17:47:20.472878Z",
          "shell.execute_reply.started": "2023-09-19T17:47:19.879832Z",
          "shell.execute_reply": "2023-09-19T17:47:20.471686Z"
        },
        "trusted": true,
        "id": "gP4lieCHuQFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_importances = pd.Series(rf_classifier.feature_importances_, index= X.columns)\n",
        "feat_importances.nsmallest(20).plot(kind='barh')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:47:20.474415Z",
          "iopub.execute_input": "2023-09-19T17:47:20.474772Z",
          "iopub.status.idle": "2023-09-19T17:47:21.074153Z",
          "shell.execute_reply.started": "2023-09-19T17:47:20.474742Z",
          "shell.execute_reply": "2023-09-19T17:47:21.072829Z"
        },
        "trusted": true,
        "id": "niIuirF9uQFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting only Important Features after Feature Importance"
      ],
      "metadata": {
        "id": "UgC7fMysuQFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From feature importance plot and correlation plot it appears that the least important features are:\n",
        "\n",
        "1- Weather_seconds\n",
        "\n",
        "2-Roundabout\n",
        "\n",
        "3-Bump\n",
        "\n",
        "4- Traffic Calming\n",
        "\n",
        "- From VIF test , the most highly correlated features with each other are:\n",
        "\n",
        "1- Temperature\n",
        "\n",
        "2- WindChill\n",
        "\n",
        "3- Humidity\n",
        "\n",
        "4- Pressure\n",
        "\n",
        "5- Start Hour\n",
        "\n",
        "6- Weather Hour\n",
        "\n",
        "7- Visibility\n",
        "\n",
        "8- Start Lat\n",
        "\n",
        "9- Start Lng"
      ],
      "metadata": {
        "id": "MxKmvp2vuQFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed , all of these columns will not be dropped , but after careful observation and analysis: The columns to be dropped are:\n",
        "\n",
        "1- Weather Seconds\n",
        "\n",
        "2- Roundabout\n",
        "\n",
        "3- Traffic calming\n",
        "\n",
        "4- Bump\n",
        "\n",
        "5- Wind Chill"
      ],
      "metadata": {
        "id": "ctKFW8MhuQFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df.drop(columns=['Roundabout','Traffic_Calming','Bump','Wind_Chill(F)','Weather_year'],inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:48:08.877606Z",
          "iopub.execute_input": "2023-09-19T17:48:08.878209Z",
          "iopub.status.idle": "2023-09-19T17:48:08.998117Z",
          "shell.execute_reply.started": "2023-09-19T17:48:08.878161Z",
          "shell.execute_reply": "2023-09-19T17:48:08.996383Z"
        },
        "trusted": true,
        "id": "JklG-tP1uQFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = sampled_df.drop('Severity', axis=1)\n",
        "y= sampled_df['Severity']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "rf_classifier = RandomForestClassifier()\n",
        "\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# print(classification_task(rf_classifier,X_train, y_train ,X_test,y_test, y_pred,'rf'))\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:53:10.314409Z",
          "iopub.execute_input": "2023-09-19T17:53:10.315316Z",
          "iopub.status.idle": "2023-09-19T17:53:10.754786Z",
          "shell.execute_reply.started": "2023-09-19T17:53:10.315270Z",
          "shell.execute_reply": "2023-09-19T17:53:10.753177Z"
        },
        "trusted": true,
        "id": "HZYAmQquuQFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking if Target is Imbalanced\n",
        "\n",
        "it's essential to assess the balance of the target variable, especially in classification tasks. An imbalanced target variable occurs when one class (or category) of the target significantly outnumbers the other(s). This imbalance can lead to various challenges and problems that can affect model training, evaluation, and generalization.\n",
        "\n",
        "Reasons for Checking if the Target is Imbalanced:\n",
        "\n",
        "1. **Model Bias**: Imbalanced target variables can lead to model bias, where the model becomes excessively biased toward the majority class. In such cases, the model may struggle to correctly predict instances from the minority class.\n",
        "\n",
        "2. **Poor Generalization**: Imbalanced data can hinder a model's ability to generalize well to new, unseen data. The model may perform well on the majority class during training but generalize poorly to the minority class, leading to suboptimal overall performance.\n",
        "\n",
        "3. **Misleading Evaluation Metrics**: Traditional accuracy is not a reliable performance metric when dealing with imbalanced data because a model that predicts the majority class all the time can still achieve high accuracy. This can lead to a false sense of model performance.\n",
        "\n",
        "4. **Rare Class Detection**: In applications where the minority class is of particular interest (e.g., fraud detection or disease diagnosis), an imbalanced dataset can make it challenging to detect rare events accurately.\n",
        "\n",
        "5. **Costly Errors**: In some scenarios, errors made on the minority class may be more costly or critical than errors on the majority class. Imbalanced data can lead to the misallocation of resources and attention.\n",
        "\n",
        "Problems Caused by Imbalanced Target Variables:\n",
        "\n",
        "1. **Class Imbalance Problem**: The primary problem is the class imbalance itself. The model tends to be biased toward the majority class, leading to poor prediction performance on the minority class.\n",
        "\n",
        "2. **Loss of Information**: Imbalanced data can result in the loss of valuable information associated with the minority class. This information may be critical for decision-making or achieving a complete understanding of the problem.\n",
        "\n",
        "3. **Skewed Decision Boundaries**: Imbalanced data can cause machine learning models to create decision boundaries that favor the majority class, making it difficult to capture the complexities of the data distribution.\n",
        "\n",
        "4. **Ineffective Learning**: Imbalanced data can lead to ineffective learning, as the model may not receive enough examples of the minority class to learn its distinguishing characteristics.\n",
        "\n",
        "5. **Biased Feature Importance**: Models trained on imbalanced data may assign higher importance to features related to the majority class, potentially overlooking essential features for the minority class.\n",
        "\n",
        "6. **Limited Real-World Applicability**: Models trained on imbalanced data may not perform well in real-world scenarios, where class distributions may be different from the training data.\n",
        "\n",
        "In summary, checking for imbalanced target variables is crucial in machine learning because it can significantly impact model performance, lead to biased predictions, and affect the generalization of models to new data. Awareness of class imbalances allows data scientists to take appropriate steps, such as resampling techniques or using alternative evaluation metrics, to address these challenges and build more robust models."
      ],
      "metadata": {
        "id": "c1CJqFw7uQFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sevrity_valueCounts=sampled_df['Severity'].value_counts().reset_index()\n",
        "sevrity_valueCounts"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:51:52.194833Z",
          "iopub.execute_input": "2023-09-19T17:51:52.196160Z",
          "iopub.status.idle": "2023-09-19T17:51:52.215968Z",
          "shell.execute_reply.started": "2023-09-19T17:51:52.196113Z",
          "shell.execute_reply": "2023-09-19T17:51:52.214858Z"
        },
        "trusted": true,
        "id": "6-SyMsTruQFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 3))\n",
        "plt.bar(sevrity_valueCounts['index'], sevrity_valueCounts['Severity'], color='skyblue')\n",
        "plt.xlabel('Severity')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Severity Distribution')\n",
        "plt.xticks(sevrity_valueCounts['Severity'])\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-19T17:52:18.651154Z",
          "iopub.execute_input": "2023-09-19T17:52:18.651737Z",
          "iopub.status.idle": "2023-09-19T17:52:18.971571Z",
          "shell.execute_reply.started": "2023-09-19T17:52:18.651694Z",
          "shell.execute_reply": "2023-09-19T17:52:18.970223Z"
        },
        "trusted": true,
        "id": "VeXcDVNRuQFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Target is Imbalanced"
      ],
      "metadata": {
        "id": "b0mRC1VFuQFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Imbalanced target"
      ],
      "metadata": {
        "id": "Fy0ZOc0JuQFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trying out the accuracies using a baseline model first to compare which balancing technique will be the most efficient"
      ],
      "metadata": {
        "id": "TpBw2qH3uQFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.205309Z",
          "iopub.status.idle": "2023-09-18T17:34:11.205747Z",
          "shell.execute_reply": "2023-09-18T17:34:11.205565Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.205546Z"
        },
        "trusted": true,
        "id": "M4KpbbmkuQFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "dt_classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.207156Z",
          "iopub.status.idle": "2023-09-18T17:34:11.208004Z",
          "shell.execute_reply": "2023-09-18T17:34:11.207812Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.207791Z"
        },
        "trusted": true,
        "id": "Vz1HbSVpuQFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = dt_classifier.predict(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.209768Z",
          "iopub.status.idle": "2023-09-18T17:34:11.210705Z",
          "shell.execute_reply": "2023-09-18T17:34:11.210409Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.210380Z"
        },
        "trusted": true,
        "id": "Au9-Msu2uQFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.212116Z",
          "iopub.status.idle": "2023-09-18T17:34:11.212676Z",
          "shell.execute_reply": "2023-09-18T17:34:11.212404Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.212379Z"
        },
        "trusted": true,
        "id": "LqGIasGHuQFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.214321Z",
          "iopub.status.idle": "2023-09-18T17:34:11.214895Z",
          "shell.execute_reply": "2023-09-18T17:34:11.214625Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.214600Z"
        },
        "trusted": true,
        "id": "LY7pTy03uQFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.216397Z",
          "iopub.status.idle": "2023-09-18T17:34:11.216971Z",
          "shell.execute_reply": "2023-09-18T17:34:11.216701Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.216675Z"
        },
        "trusted": true,
        "id": "_XtkAjkAuQFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trying different balancing techniques and observing confusion matrix and accuracy score of each one"
      ],
      "metadata": {
        "id": "gMFh--PPuQFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Undersampling using Random Under Sampling**\n",
        "\n",
        "Undersampling is a widely used technique in the field of imbalanced classification problems. It involves reducing the size of the majority class by randomly removing some of its instances to balance the class distribution. One specific method for undersampling is the \"Random Undersampler.\" This approach randomly selects a subset of the majority class's instances to match the size of the minority class. The goal is to create a balanced dataset that can be used to train machine learning models effectively.\n",
        "\n",
        "In summary, Random Undersampler is a valuable tool for addressing class imbalance in imbalanced classification problems. It helps create a balanced dataset by randomly selecting a subset of instances from the majority class, allowing machine learning models to perform better, especially on the minority class. However, it's essential to strike a balance between addressing class imbalance and retaining valuable information when applying this technique."
      ],
      "metadata": {
        "id": "wHuQ8K6nuQFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sam= RandomUnderSampler(random_state=0)\n",
        "X_resampled_under , y_resampled_under = sam.fit_resample(X,y)\n",
        "y_resampled_under.value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.218490Z",
          "iopub.status.idle": "2023-09-18T17:34:11.219030Z",
          "shell.execute_reply": "2023-09-18T17:34:11.218771Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.218745Z"
        },
        "trusted": true,
        "id": "u_x1JrysuQFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_resampled_under, X_test_resampled, y_resampled_under, y_test_resampled = train_test_split(X_resampled_under, y_resampled_under, test_size=0.30, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.220430Z",
          "iopub.status.idle": "2023-09-18T17:34:11.221028Z",
          "shell.execute_reply": "2023-09-18T17:34:11.220754Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.220726Z"
        },
        "trusted": true,
        "id": "ie-F1sREuQFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "dt_classifier.fit(X_resampled_under, y_resampled_under)\n",
        "y_pred = dt_classifier.predict(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.222590Z",
          "iopub.status.idle": "2023-09-18T17:34:11.223153Z",
          "shell.execute_reply": "2023-09-18T17:34:11.222894Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.222868Z"
        },
        "trusted": true,
        "id": "mDH3hmEjuQFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.224651Z",
          "iopub.status.idle": "2023-09-18T17:34:11.225185Z",
          "shell.execute_reply": "2023-09-18T17:34:11.224943Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.224917Z"
        },
        "trusted": true,
        "id": "j-x--jnIuQFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Models"
      ],
      "metadata": {
        "id": "vIt4E_QEuQFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classification_task( model,X_train_scaled, y_train ,X_test_scaled ,y_test, predic,model_name):\n",
        "\n",
        "    perf_df=pd.DataFrame({'Train_Score':model.score(X_train_scaled,y_train),\"Test_Score\":model.score(X_test_scaled,y_test),\n",
        "                       \"Precision_Score\":precision_score(y_test,predic,average='weighted'),\"Recall_Score\":recall_score(y_test,predic,average='weighted'),\n",
        "                       \"F1_Score\":f1_score(y_test,predic,average='weighted') , \"accuracy\":accuracy_score(y_test,predic)}, index=[model_name])\n",
        "    return perf_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.226615Z",
          "iopub.status.idle": "2023-09-18T17:34:11.227060Z",
          "shell.execute_reply": "2023-09-18T17:34:11.226879Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.226859Z"
        },
        "trusted": true,
        "id": "kgedTOe7uQFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## without PCA\n",
        "\n",
        "First , we are going to try implementing all the models before PCA and then after choosing the highest performing model we will then apply PCA to observe whether the accuracy has changed"
      ],
      "metadata": {
        "id": "6OMGIQQsuQFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest\n",
        "\n",
        "The random forest algorithm is an extension of the bagging method as it utilizes both bagging and feature randomness to create an uncorrelated forest of decision trees. Feature randomness, also known as feature bagging or “the random subspace method”(link resides outside ibm.com) (PDF, 121 KB), generates a random subset of features, which ensures low correlation among decision trees. This is a key difference between decision trees and random forests. While decision trees consider all the possible feature splits, random forests only select a subset of those features.\n",
        "\n",
        "Random forest algorithms have three main hyperparameters, which need to be set before training. These include node size, the number of trees, and the number of features sampled. From there, the random forest classifier can be used to solve for regression or classification problems.\n",
        "\n",
        "\n",
        "Before understanding the working of the random forest algorithm in machine learning, we must look into the ensemble learning technique. Ensemble simplymeans combining multiple models. Thus a collection of models is used to make predictions rather than an individual model.\n",
        "\n",
        "Ensemble uses two types of methods:\n",
        "\n",
        "1. Bagging– It creates a different training subset from sample training data with replacement & the final output is based on majority voting. For example,  Random Forest.\n",
        "\n",
        "2. Boosting– It combines weak learners into strong learners by creating sequential models such that the final model has the highest accuracy. For example,  ADA BOOST, XG BOOST.\n",
        "![rf image](https://av-eks-blogoptimized.s3.amazonaws.com/4661536426211ba43ea612c8e1a6a1ed4550721164.png)"
      ],
      "metadata": {
        "id": "eeoE6R4OuQFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - **Scaling**: Random Forest is not sensitive to feature scales, so scaling is not necessary.\n",
        "   - **Balancing**: While Random Forest is robust to class imbalance, balancing can still be beneficial in certain cases, but it's not always required."
      ],
      "metadata": {
        "id": "RHFxn161uQFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "rf_classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.231207Z",
          "iopub.status.idle": "2023-09-18T17:34:11.231634Z",
          "shell.execute_reply": "2023-09-18T17:34:11.231443Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.231426Z"
        },
        "trusted": true,
        "id": "pHD3DOQGuQFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "Eval_Rf= classification_task(rf_classifier,X_train, y_train ,X_test ,y_test, y_pred ,'Random Forest')\n",
        "Eval_Rf"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.233113Z",
          "iopub.status.idle": "2023-09-18T17:34:11.233546Z",
          "shell.execute_reply": "2023-09-18T17:34:11.233340Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.233322Z"
        },
        "trusted": true,
        "id": "hZf1aPgbuQFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.235079Z",
          "iopub.status.idle": "2023-09-18T17:34:11.235483Z",
          "shell.execute_reply": "2023-09-18T17:34:11.235297Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.235279Z"
        },
        "trusted": true,
        "id": "zozYVzaiuQFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees\n",
        "\n",
        "\n",
        "A decision tree is a non-parametric supervised learning algorithm for classification and regression tasks. It has a hierarchical tree structure consisting of a root node, branches, internal nodes, and leaf nodes. Decision trees are used for classification and regression tasks, providing easy-to-understand models.\n",
        "\n",
        "A decision tree is a hierarchical model used in decision support that depicts decisions and their potential outcomes, incorporating chance events, resource expenses, and utility. This algorithmic model utilizes conditional control statements and is non-parametric, supervised learning, useful for both classification and regression tasks. The tree structure is comprised of a root node, branches, internal nodes, and leaf nodes, forming a hierarchical, tree-like structure.\n",
        "\n",
        "It is a tool that has applications spanning several different areas. Decision trees can be used for classification as well as regression problems. The name itself suggests that it uses a flowchart like a tree structure to show the predictions that result from a series of feature-based splits. It starts with a root node and ends with a decision made by leaves\n",
        "\n",
        "![Dt](https://av-eks-blogoptimized.s3.amazonaws.com/498772.png)\n"
      ],
      "metadata": {
        "id": "wFFMjZ2TuQFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   - **Scaling**: Decision Trees are not sensitive to feature scales, so scaling is not necessary.\n",
        "   - **Balancing**: Decision Trees are not inherently sensitive to class imbalance. However, balancing can still be helpful in some scenarios."
      ],
      "metadata": {
        "id": "h6f9b2y0uQFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_resampled_under, y_resampled_under)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.237132Z",
          "iopub.status.idle": "2023-09-18T17:34:11.237541Z",
          "shell.execute_reply": "2023-09-18T17:34:11.237351Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.237333Z"
        },
        "trusted": true,
        "id": "Uwxr0XUNuQFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rf_classifier.predict(X_test_resampled)\n",
        "Eval_Dt=classification_task(dt_classifier,X_resampled_under, y_resampled_under ,X_test_resampled ,y_test_resampled, y_pred,'Decision Trees')\n",
        "Eval_Dt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.239190Z",
          "iopub.status.idle": "2023-09-18T17:34:11.239595Z",
          "shell.execute_reply": "2023-09-18T17:34:11.239406Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.239388Z"
        },
        "trusted": true,
        "id": "9aScA2ceuQFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test_resampled, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test_resampled, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.240904Z",
          "iopub.status.idle": "2023-09-18T17:34:11.241362Z",
          "shell.execute_reply": "2023-09-18T17:34:11.241171Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.241151Z"
        },
        "trusted": true,
        "id": "ayMyXR6tuQFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN\n",
        "The K-Nearest Neighbor (KNN) algorithm is a popular machine learning technique used for classification and regression tasks. It relies on the idea that similar data points tend to have similar labels or values.\n",
        "\n",
        "During the training phase, the KNN algorithm stores the entire training dataset as a reference. When making predictions, it calculates the distance between the input data point and all the training examples, using a chosen distance metric such as Euclidean distance.\n",
        "\n",
        "Next, the algorithm identifies the K nearest neighbors to the input data point based on their distances. In the case of classification, the algorithm assigns the most common class label among the K neighbors as the predicted label for the input data point. For regression, it calculates the average or weighted average of the target values of the K neighbors to predict the value for the input data point\n",
        "![KNN](https://av-eks-blogoptimized.s3.amazonaws.com/scenario2.png)"
      ],
      "metadata": {
        "id": "Nw7pyKAbuQFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   - **Scaling**: KNN is highly sensitive to feature scales, so scaling is necessary for this algorithm.\n",
        "   - **Balancing**: Balancing the target classes can help improve KNN's performance, especially when the class imbalance is substantial."
      ],
      "metadata": {
        "id": "vyLUhAt0uQFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling the Balanced Data using Standard Scalar"
      ],
      "metadata": {
        "id": "oIoy3WF2uQFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_resampled_under)\n",
        "X_test_scaled = scaler.transform(X_test_resampled)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.242768Z",
          "iopub.status.idle": "2023-09-18T17:34:11.243212Z",
          "shell.execute_reply": "2023-09-18T17:34:11.243008Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.242989Z"
        },
        "trusted": true,
        "id": "JSd7umcKuQFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_classifier = KNeighborsClassifier()\n",
        "knn_classifier.fit(X_train_scaled, y_resampled_under)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.245891Z",
          "iopub.status.idle": "2023-09-18T17:34:11.246328Z",
          "shell.execute_reply": "2023-09-18T17:34:11.246143Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.246123Z"
        },
        "trusted": true,
        "id": "Fz8EK5eluQFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = knn_classifier.predict(X_test_scaled)\n",
        "Eval_KNN=classification_task(knn_classifier,X_train_scaled, y_resampled_under ,X_test_scaled ,y_test_resampled, y_pred,'KNN')\n",
        "Eval_KNN"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.247849Z",
          "iopub.status.idle": "2023-09-18T17:34:11.248276Z",
          "shell.execute_reply": "2023-09-18T17:34:11.248087Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.248068Z"
        },
        "trusted": true,
        "id": "gHYYmfkuuQFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test_resampled, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test_resampled, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.249653Z",
          "iopub.status.idle": "2023-09-18T17:34:11.250067Z",
          "shell.execute_reply": "2023-09-18T17:34:11.249891Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.249871Z"
        },
        "trusted": true,
        "id": "0B9HG6gWuQFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes\n",
        "It is a classification technique based on Bayes’ Theorem with an independence assumption among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
        "\n",
        "The Naïve Bayes classifier is a popular supervised machine learning algorithm used for classification tasks such as text classification. It belongs to the family of generative learning algorithms, which means that it models the distribution of inputs for a given class or category. This approach is based on the assumption that the features of the input data are conditionally independent given the class, allowing the algorithm to make predictions quickly and accurately.\n",
        "\n",
        "In statistics, naive Bayes classifiers are considered as simple probabilistic classifiers that apply Bayes’ theorem. This theorem is based on the probability of a hypothesis, given the data and some prior knowledge. The naive Bayes classifier assumes that all features in the input data are independent of each other, which is often not true in real-world scenarios. However, despite this simplifying assumption, the naive Bayes classifier is widely used because of its efficiency and good performance in many real-world applications.\n",
        "\n",
        "Moreover, it is worth noting that naive Bayes classifiers are among the simplest Bayesian network models, yet they can achieve high accuracy levels when coupled with kernel density estimation. This technique involves using a kernel function to estimate the probability density function of the input data, allowing the classifier to improve its performance in complex scenarios where the data distribution is not well-defined. As a result, the naive Bayes classifier is a powerful tool in machine learning, particularly in text classification, spam filtering, and sentiment analysis, among others.\n",
        "\n",
        "For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
        "\n",
        "An NB model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
        "\n",
        "Bayes theorem provides a way of computing posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below\n",
        "![Naiive Bayes](https://av-eks-blogoptimized.s3.amazonaws.com/Bayes_rule-300x172-300x172-111664.png)"
      ],
      "metadata": {
        "id": "EyO_ap3IuQFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - **Scaling**: Naive Bayes is not sensitive to feature scales, so scaling is not typically necessary.\n",
        "   - **Balancing**: Naive Bayes can work well with imbalanced datasets, but it can benefit from balancing."
      ],
      "metadata": {
        "id": "SLzO8e8puQFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_resampled_under, y_resampled_under)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.251685Z",
          "iopub.status.idle": "2023-09-18T17:34:11.252065Z",
          "shell.execute_reply": "2023-09-18T17:34:11.251898Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.251881Z"
        },
        "trusted": true,
        "id": "XRT3ZfcnuQFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nb_classifier.predict(X_test_scaled)\n",
        "Eval_nb=classification_task(nb_classifier,X_resampled_under, y_resampled_under ,X_test_resampled ,y_test_resampled, y_pred,'Naive Bayes')\n",
        "Eval_nb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.253659Z",
          "iopub.status.idle": "2023-09-18T17:34:11.254614Z",
          "shell.execute_reply": "2023-09-18T17:34:11.254351Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.254330Z"
        },
        "trusted": true,
        "id": "VFj5bCeRuQFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test_resampled, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test_resampled, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.256205Z",
          "iopub.status.idle": "2023-09-18T17:34:11.256666Z",
          "shell.execute_reply": "2023-09-18T17:34:11.256440Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.256419Z"
        },
        "trusted": true,
        "id": "KdPtM4anuQFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost\n",
        "\n",
        "XGBoost is an ensemble learning method. Sometimes, it may not be sufficient to rely upon the results of just one machine learning model. Ensemble learning offers a systematic solution to combine the predictive power of multiple learners. The resultant is a single model which gives the aggregated output from several models\n",
        "The models that form the ensemble, also known as base learners, could be either from the same learning algorithm or different learning algorithms. Bagging and boosting are two widely used ensemble learners. Though these two techniques can be used with several statistical models, the most predominant usage has been with decision trees.\n",
        "\n",
        "Let’s briefly discuss bagging before taking a more detailed look at the concept of gradient boosting.\n",
        "1. Bagging\n",
        "\n",
        "While decision trees are one of the most easily interpretable models, they exhibit highly variable behavior. Consider a single training dataset that we randomly split into two parts. Now, let’s use each part to train a decision tree in order to obtain two models.\n",
        "\n",
        "When we fit both these models, they would yield different results. Decision trees are said to be associated with high variance due to this behavior. Bagging or boosting aggregation helps to reduce the variance in any learner. Several decision trees which are generated in parallel, form the base learners of bagging technique. Data sampled with replacement is fed to these learners for training. The final prediction is the averaged output from all the learners.\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "In boosting, the trees are built sequentially such that each subsequent tree aims to reduce the errors of the previous tree. Each tree learns from its predecessors and updates the residual errors. Hence, the tree that grows next in the sequence will learn from an updated version of the residuals.\n",
        "\n",
        "The base learners in boosting are weak learners in which the bias is high, and the predictive power is just a tad better than random guessing. Each of these weak learners contributes some vital information for prediction, enabling the boosting technique to produce a strong learner by effectively combining these weak learners. The final strong learner brings down both the bias and the variance.\n",
        "\n",
        "In contrast to bagging techniques like Random Forest, in which trees are grown to their maximum extent, boosting makes use of trees with fewer splits. Such small trees, which are not very deep, are highly interpretable. Parameters like the number of trees or iterations, the rate at which the gradient boosting learns, and the depth of the tree, could be optimally selected through validation techniques like k-fold cross validation. Having a large number of trees might lead to overfitting. So, it is necessary to carefully choose the stopping criteria for boosting.\n",
        "\n",
        "![naiive bayes 2](https://media.geeksforgeeks.org/wp-content/uploads/20210707140912/Bagging.png)"
      ],
      "metadata": {
        "id": "NCrLoBP-uQFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - **Scaling**: XGBoost is not sensitive to feature scales, so scaling is not necessary.\n",
        "   - **Balancing**: XGBoost can handle class imbalance to some extent, but balancing the target classes might still be useful in extreme cases."
      ],
      "metadata": {
        "id": "LlDsi0qluQFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(y_train-1).unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.257820Z",
          "iopub.status.idle": "2023-09-18T17:34:11.258229Z",
          "shell.execute_reply": "2023-09-18T17:34:11.258052Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.258033Z"
        },
        "trusted": true,
        "id": "TuqD4vfhuQFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_classifier = XGBClassifier(objective='multi:softmax',random_state=42)\n",
        "xgb_classifier.fit(X_resampled_under, y_resampled_under-1 )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.259824Z",
          "iopub.status.idle": "2023-09-18T17:34:11.260231Z",
          "shell.execute_reply": "2023-09-18T17:34:11.260054Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.260035Z"
        },
        "trusted": true,
        "id": "qvcpVNsKuQFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = xgb_classifier.predict(X_test_resampled)\n",
        "Eval_xgb=classification_task(xgb_classifier,X_resampled_under, y_resampled_under-1 ,X_test_resampled,y_test_resampled-1, y_pred,'XGBoost')\n",
        "Eval_xgb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.261814Z",
          "iopub.status.idle": "2023-09-18T17:34:11.262219Z",
          "shell.execute_reply": "2023-09-18T17:34:11.262042Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.262024Z"
        },
        "trusted": true,
        "id": "vsnA78TluQFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test_resampled-1, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test_resampled-1, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test_resampled-1, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.263580Z",
          "iopub.status.idle": "2023-09-18T17:34:11.264000Z",
          "shell.execute_reply": "2023-09-18T17:34:11.263820Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.263800Z"
        },
        "trusted": true,
        "id": "zrNnAFEuuQFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADABoost\n",
        "AdaBoost, also called Adaptive Boosting, is a technique in Machine Learning used as an Ensemble Method. The most common estimator used with AdaBoost is decision trees with one level which means Decision trees with only 1 split. These trees are also called Decision Stumps.\n",
        "\n",
        "What this algorithm does is that it builds a model and gives equal weights to all the data points. It then assigns higher weights to points that are wrongly classified. Now all the points with higher weights are given more importance in the next model. It will keep training models until and unless a lower error is received.\n",
        "Let’s take an example to understand this, suppose you built a decision tree algorithm on the Titanic dataset, and from there, you get an accuracy of 80%. After this, you apply a different algorithm and check the accuracy, and it comes out to be 75% for KNN and 70% for Linear Regression.\n",
        "\n",
        "We see the accuracy differs when we build a different model on the same dataset. But what if we use combinations of all these algorithms to make the final prediction? We’ll get more accurate results by taking the average of the results from these models. We can increase the prediction power in this way.\n",
        "\n",
        "![Ada boots](https://av-eks-blogoptimized.s3.amazonaws.com/159381.png)"
      ],
      "metadata": {
        "id": "prYOmOJTuQFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - **Scaling**: AdaBoost is not sensitive to feature scales, so scaling is not necessary.\n",
        "   - **Balancing**: AdaBoost can benefit from balanced classes, particularly when using weak learners that are sensitive to class imbalance."
      ],
      "metadata": {
        "id": "ITrfNw5-uQFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_classifier = AdaBoostClassifier(random_state=42)\n",
        "ada_classifier.fit(X_resampled_under, y_resampled_under)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.265939Z",
          "iopub.status.idle": "2023-09-18T17:34:11.266325Z",
          "shell.execute_reply": "2023-09-18T17:34:11.266154Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.266137Z"
        },
        "trusted": true,
        "id": "xD97gOrUuQFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = ada_classifier.predict(X_test_resampled)\n",
        "Eval_ada=classification_task(ada_classifier,X_resampled_under, y_resampled_under ,X_test_resampled,y_test_resampled, y_pred,'ADABoost')\n",
        "Eval_ada"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.268041Z",
          "iopub.status.idle": "2023-09-18T17:34:11.268566Z",
          "shell.execute_reply": "2023-09-18T17:34:11.268369Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.268349Z"
        },
        "trusted": true,
        "id": "87fvKvY1uQFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test_resampled, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test_resampled, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.269683Z",
          "iopub.status.idle": "2023-09-18T17:34:11.270107Z",
          "shell.execute_reply": "2023-09-18T17:34:11.269927Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.269908Z"
        },
        "trusted": true,
        "id": "LK6S_6mJuQFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balanced Bagging Classifier\n",
        "A Balanced Bagging Classifier is an ensemble machine learning technique that combines the principles of both bagging and resampling to address class imbalance in a dataset. It is particularly useful when you have a classification problem with imbalanced classes, where one class significantly outnumbers the other(s).\n",
        "\n",
        "Here's an explanation of the key components and concepts behind a Balanced Bagging Classifier:\n",
        "\n",
        "1. **Bagging (Bootstrap Aggregating)**:\n",
        "   - Bagging is a technique that involves creating multiple subsets (bootstrap samples) of the original dataset by randomly sampling with replacement. Each subset is used to train a separate base classifier.\n",
        "   - These base classifiers can be any classification algorithm, such as decision trees, random forests, or support vector machines.\n",
        "\n",
        "2. **Class Imbalance**:\n",
        "   - In the context of imbalanced datasets, the term \"class imbalance\" refers to the situation where one class (the minority class) has significantly fewer instances than another class (the majority class).\n",
        "   - Traditional bagging techniques do not address class imbalance directly and may produce models biased towards the majority class.\n",
        "\n",
        "3. **Balancing the Subsets**:\n",
        "   - In a Balanced Bagging Classifier, extra steps are taken to address class imbalance. When creating the bootstrap samples, each sample is constructed to have a balanced representation of both the majority and minority classes.\n",
        "   - This can be achieved by oversampling the minority class (adding more instances of the minority class) or undersampling the majority class (removing some instances of the majority class) within each bootstrap sample.\n",
        "\n",
        "4. **Aggregation**:\n",
        "   - After training base classifiers on the balanced bootstrap samples, predictions from each base classifier are aggregated to make the final prediction.\n",
        "   - The most common aggregation method is majority voting for classification problems. Each base classifier \"votes\" for a class, and the class with the most votes is selected as the final prediction.\n",
        "\n",
        "Benefits of Balanced Bagging Classifier:\n",
        "\n",
        "- **Mitigating Class Imbalance**: By ensuring that each bootstrap sample is balanced, a Balanced Bagging Classifier reduces the risk of creating models that are heavily biased toward the majority class.\n",
        "\n",
        "- **Improving Generalization**: Ensemble methods like bagging often improve model generalization and reduce overfitting by averaging predictions from multiple models.\n",
        "\n",
        "- **Handling Noisy Data**: It can be effective at handling noisy datasets, as base classifiers might focus more on noisy instances when trained on balanced subsets.\n",
        "\n",
        "- **Compatibility**: It can be used with various base classifiers, making it versatile and applicable to different types of classification problems.\n",
        "\n",
        "However, it's important to note that the choice of the number of base classifiers and the balance between oversampling and undersampling should be tuned carefully to achieve the best results for your specific dataset and problem. Additionally, Balanced Bagging Classifier is just one of several techniques to address class imbalance, and its effectiveness depends on the nature of the data and the problem at hand.\n",
        "\n",
        "![bbc](https://miro.medium.com/v2/resize:fit:582/1*HIUTuk72aqWIOZ-VBhclHQ.jpeg)"
      ],
      "metadata": {
        "id": "lPBlHJN8uQFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  - **Scaling**: Balanced Bagging is an ensemble technique, and whether or not to scale features depends on the base estimator used within the ensemble.  \n",
        "  - **Balancing**: Balanced Bagging is designed to handle imbalanced datasets, so it can be effective for addressing class imbalance."
      ],
      "metadata": {
        "id": "3O7K5SFluQFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "balanced_bagging = BalancedBaggingClassifier(base_estimator=base_estimator, n_estimators=100, random_state=42)\n",
        "\n",
        "balanced_bagging.fit(X_train, y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.273282Z",
          "iopub.status.idle": "2023-09-18T17:34:11.273704Z",
          "shell.execute_reply": "2023-09-18T17:34:11.273525Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.273505Z"
        },
        "trusted": true,
        "id": "_tht8cv3uQFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = balanced_bagging.predict(X_test)\n",
        "Eval_bbc=classification_task(balanced_bagging,X_train, y_train ,X_test,y_test, y_pred,'BBC')\n",
        "Eval_bbc"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.275088Z",
          "iopub.status.idle": "2023-09-18T17:34:11.275567Z",
          "shell.execute_reply": "2023-09-18T17:34:11.275348Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.275328Z"
        },
        "trusted": true,
        "id": "vDgDoL9yuQFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.277760Z",
          "iopub.status.idle": "2023-09-18T17:34:11.278186Z",
          "shell.execute_reply": "2023-09-18T17:34:11.278006Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.277987Z"
        },
        "trusted": true,
        "id": "Px2DWacYuQFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "\n",
        "This type of statistical model (also known as logit model) is often used for classification and predictive analytics. Logistic regression estimates the probability of an event occurring, such as voted or didn’t vote, based on a given dataset of independent variables. Since the outcome is a probability, the dependent variable is bounded between 0 and 1. In logistic regression, a logit transformation is applied on the odds—that is, the probability of success divided by the probability of failure. This is also commonly known as the log odds, or the natural logarithm of odds, and this logistic function is represented by the following formulas:\n",
        "\n",
        "Logit(pi) = 1/(1+ exp(-pi))\n",
        "\n",
        "ln(pi/(1-pi)) = Beta_0 + Beta_1*X_1 + … + B_k*K_k\n",
        "In this logistic regression equation, logit(pi) is the dependent or response variable and x is the independent variable. The beta parameter, or coefficient, in this model is commonly estimated via maximum likelihood estimation (MLE). This method tests different values of beta through multiple iterations to optimize for the best fit of log odds. All of these iterations produce the log likelihood function, and logistic regression seeks to maximize this function to find the best parameter estimate. Once the optimal coefficient (or coefficients if there is more than one independent variable) is found, the conditional probabilities for each observation can be calculated, logged, and summed together to yield a predicted probability. For binary classification, a probability less than .5 will predict 0 while a probability greater than 0 will predict 1.  After the model has been computed, it’s best practice to evaluate the how well the model predicts the dependent variable, which is called goodness of fit. The Hosmer–Lemeshow test is a popular method to assess model fit.\n",
        "\n",
        "\n",
        "![logreg](https://images.spiceworks.com/wp-content/uploads/2022/04/11040522/46-4.png)"
      ],
      "metadata": {
        "id": "Zl5-lsftuQFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - **Scaling**: It is generally a good practice to scale features when using logistic regression, as it is sensitive to the scale of input features.\n",
        "   - **Balancing**: Logistic regression can be affected by class imbalance, so balancing the target classes, especially when the imbalance is significant, is advisable."
      ],
      "metadata": {
        "id": "PyGeSYBxuQFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression = LogisticRegression(random_state=42)\n",
        "logistic_regression.fit(X_train_scaled, y_resampled_under)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.279321Z",
          "iopub.status.idle": "2023-09-18T17:34:11.279763Z",
          "shell.execute_reply": "2023-09-18T17:34:11.279571Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.279551Z"
        },
        "trusted": true,
        "id": "Kler2vlluQFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = logistic_regression.predict(X_test_scaled)\n",
        "Eval_logreg=classification_task(logistic_regression,X_train_scaled, y_resampled_under ,X_test_scaled,y_test_resampled, y_pred,'logreg')\n",
        "Eval_logreg"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.281644Z",
          "iopub.status.idle": "2023-09-18T17:34:11.282074Z",
          "shell.execute_reply": "2023-09-18T17:34:11.281889Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.281869Z"
        },
        "trusted": true,
        "id": "8TYLZySPuQFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test_resampled, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test_resampled, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.283786Z",
          "iopub.status.idle": "2023-09-18T17:34:11.284251Z",
          "shell.execute_reply": "2023-09-18T17:34:11.284063Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.284043Z"
        },
        "trusted": true,
        "id": "tpyVV6PkuQFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting\n",
        "A Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.\n",
        "It simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n",
        "\n",
        "Voting Classifier supports two types of votings.\n",
        "\n",
        "    Hard Voting: In hard voting, the predicted output class is a class with the highest majority of votes i.e the class which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction.\n",
        "    \n",
        "    Soft Voting: In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier.\n",
        "\n",
        "![Voting](https://miro.medium.com/v2/resize:fit:1400/1*2KMR70n4sqEkMsbyY63LBw.png)"
      ],
      "metadata": {
        "id": "1JSup0B5uQFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   - **Scaling**: Scaling requirements depend on the individual models within the ensemble.\n",
        "   - **Balancing**: Balancing is often advisable for ensemble models, especially when any of the individual base models require it."
      ],
      "metadata": {
        "id": "2rrHHTXVuQFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting_classifier = VotingClassifier(estimators=[\n",
        "    ('ada', ada_classifier),\n",
        "    ('dt_classifier',dt_classifier),\n",
        "    ('rf_classifier',rf_classifier),\n",
        "    ('xgb', xgb_classifier),\n",
        "    ('logreg',logistic_regression),\n",
        "    ('bbc',balanced_bagging)\n",
        "], voting='soft')\n",
        "\n",
        "voting_classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.286086Z",
          "iopub.status.idle": "2023-09-18T17:34:11.286564Z",
          "shell.execute_reply": "2023-09-18T17:34:11.286343Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.286323Z"
        },
        "trusted": true,
        "id": "6nbnOuaLuQFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = voting_classifier.predict(X_test)\n",
        "\n",
        "Eval_voting=classification_task(voting_classifier,X_train, y_train ,X_test,y_test, y_pred,'voting')\n",
        "Eval_voting"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.288292Z",
          "iopub.status.idle": "2023-09-18T17:34:11.288732Z",
          "shell.execute_reply": "2023-09-18T17:34:11.288550Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.288530Z"
        },
        "trusted": true,
        "id": "aA9LaUIUuQFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-18T17:34:11.295960Z",
          "iopub.status.idle": "2023-09-18T17:34:11.296602Z",
          "shell.execute_reply": "2023-09-18T17:34:11.296308Z",
          "shell.execute_reply.started": "2023-09-18T17:34:11.296277Z"
        },
        "trusted": true,
        "id": "37ZHa02luQFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA on Highest Performing Model"
      ],
      "metadata": {
        "id": "HtzxBAP2uQFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction (PCA)\n",
        "PCA, or Principal Component Analysis, is a dimensionality reduction technique used in machine learning and data analysis. Its primary goal is to reduce the number of features (variables) in a dataset while preserving as much of the essential information as possible. PCA achieves this by transforming the original features into a new set of linearly uncorrelated variables called principal components.\n",
        "\n",
        "Here's an overview of how PCA works:\n",
        "\n",
        "1. **Centering the Data**:\n",
        "   - PCA begins by centering the data. This involves subtracting the mean of each feature from all data points. Centering ensures that the new coordinate system has its origin at the centroid of the data.\n",
        "\n",
        "2. **Covariance Matrix**:\n",
        "   - PCA then computes the covariance matrix of the centered data. The covariance matrix captures the relationships between the features. Off-diagonal elements in the covariance matrix represent how different features vary together.\n",
        "\n",
        "3. **Eigenvalue Decomposition**:\n",
        "   - The next step is to perform eigenvalue decomposition on the covariance matrix. This decomposition results in a set of eigenvalues and their corresponding eigenvectors.\n",
        "   - Eigenvalues represent the amount of variance explained by each principal component. They are sorted in descending order, so the first eigenvalue explains the most variance, the second eigenvalue explains the second most, and so on.\n",
        "   - Eigenvectors are the directions (linear combinations of the original features) along which the data varies the most. Each eigenvector corresponds to a principal component.\n",
        "\n",
        "4. **Selecting Principal Components**:\n",
        "   - You can choose to retain a certain number of principal components based on the amount of variance you want to preserve. This is often specified as a percentage of the total variance.\n",
        "   - Selecting fewer principal components means reducing the dimensionality of the data.\n",
        "\n",
        "5. **Projection**:\n",
        "   - Finally, the original data is projected onto the selected principal components. This projection transforms the data into a new coordinate system where the axes are the principal components.\n",
        "   - Each data point is represented by its coordinates in this new space, with the coordinates along the principal components indicating how much the data point contributes to each component.\n",
        "\n",
        "PCA has several practical applications:\n",
        "\n",
        "- **Dimensionality Reduction**: PCA is commonly used to reduce the number of features in a dataset, which can lead to simpler models, reduced computational complexity, and improved model performance.\n",
        "\n",
        "- **Data Visualization**: PCA can be employed to visualize high-dimensional data in lower dimensions (e.g., 2D or 3D) while preserving as much variance as possible. This aids in exploratory data analysis.\n",
        "\n",
        "- **Noise Reduction**: By focusing on the top principal components, which capture the most important patterns in the data, PCA can help reduce the impact of noise in the data.\n",
        "\n",
        "- **Feature Engineering**: PCA can be used to create new features or representations of data that are more informative and less correlated than the original features.\n",
        "\n",
        "It's important to note that PCA assumes that the data is linear and that the principal components are orthogonal (uncorrelated). While PCA is a powerful technique, it may not always be the best choice, especially when non-linear relationships exist in the data. In such cases, nonlinear dimensionality reduction techniques like t-SNE or autoencoders may be more appropriate.\n",
        "![pca](https://devopedia.org/images/article/139/4543.1548137789.jpg)"
      ],
      "metadata": {
        "id": "h3sXn5ESuQFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This plot will help determine the best no of components that will help us catch the max number of variations"
      ],
      "metadata": {
        "id": "qQxAGBt5uQFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = StandardScaler()\n",
        "df_scaled= sc.fit_transform(sampled_df)\n",
        "pca = PCA().fit(df_scaled)"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-17T18:19:15.049742Z",
          "iopub.status.busy": "2023-09-17T18:19:15.049196Z",
          "iopub.status.idle": "2023-09-17T18:19:15.728425Z",
          "shell.execute_reply": "2023-09-17T18:19:15.726823Z",
          "shell.execute_reply.started": "2023-09-17T18:19:15.049702Z"
        },
        "trusted": true,
        "id": "BEtJZwRwuQFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "xi = np.arange(1, 58, step=1)\n",
        "y = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "plt.ylim(0.0,1.1)\n",
        "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
        "\n",
        "plt.xlabel('Number of Components')\n",
        "plt.xticks(np.arange(0, 58, step=1))\n",
        "plt.ylabel('Cumulative variance (%)')\n",
        "plt.title('The number of components needed to explain variance')\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "ax.grid(axis='x')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "iUpXWpbeuQFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**: From the plot , to get 95% of variance explained 39 principal components are needed."
      ],
      "metadata": {
        "id": "ptfdKh-WuQFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components = 39)\n",
        "\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "principalDf = pd.DataFrame(data=X_train, columns=[f'principal component {i+1}' for i in range(39)])\n",
        "\n",
        "principalDf"
      ],
      "metadata": {
        "trusted": true,
        "id": "N2CkccosuQFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "trusted": true,
        "id": "Et0xi121uQFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the explained variance ratio is a crucial concept that helps understand how much of the total variance in the original data is retained or explained by each principal component. It provides insights into the relative importance of each principal component in capturing the data's overall variability. The explained variance ratio is often used to determine how many principal components to retain when reducing the dimensionality of the data.\n"
      ],
      "metadata": {
        "id": "ZVtG7COruQFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing PCA when n_components = 2\n",
        "\n",
        "\n",
        "Visualizing PCA components with a scatter plot is a helpful way to gain insights into the relationships and structures within the data after dimensionality reduction. While PCA is primarily a technique for reducing the dimensionality of data and capturing its variance, scatter plots of PCA components offer several advantages:\n",
        "\n",
        "1. **Data Exploration**:\n",
        "   - Scatter plots allow you to explore the data in a reduced-dimensional space, making it easier to detect patterns, clusters, and outliers.\n",
        "   - By visualizing the data, you can identify interesting groupings or separations that may not have been apparent in the original high-dimensional space.\n",
        "\n",
        "2. **Dimensionality Reduction Assessment**:\n",
        "   - Scatter plots help you assess the effectiveness of dimensionality reduction. You can check if the reduced dimensions (principal components) capture the essential variations in the data.\n",
        "   - Scatter plots provide a visual confirmation of whether the data retains its distinguishing characteristics in the lower-dimensional space.\n",
        "\n",
        "3. **Cluster Detection**:\n",
        "   - Clusters or groupings of data points that were not immediately evident in the original feature space may become apparent in a scatter plot of principal components.\n",
        "\n",
        "4. **Outlier Detection**:\n",
        "   - Outliers, which can be challenging to spot in high-dimensional spaces, often become more noticeable in scatter plots.\n",
        "   - Identifying outliers is crucial for quality control, fraud detection, and other applications.\n",
        "\n",
        "5. **Feature Importance Assessment**:\n",
        "   - You can assess the importance of original features in contributing to principal components by examining the scatter plot. Features that have a significant impact on the data's variance are often strongly correlated with principal components.\n",
        "\n",
        "In summary, scatter plots of PCA components are a valuable tool for exploring, understanding, and interpreting the structure of your data in a reduced-dimensional space. They facilitate data analysis, model building, and decision-making processes by providing visual insights into the relationships between data points and the effectiveness of dimensionality reduction."
      ],
      "metadata": {
        "id": "xCA6SqKjuQFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = sampled_df.drop('Severity', axis=1)\n",
        "y= sampled_df['Severity']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components = 2)\n",
        "\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "principalDf = pd.DataFrame(data=X_train, columns=['principal component 1', 'principal component 2'])\n",
        "\n",
        "principalDf"
      ],
      "metadata": {
        "trusted": true,
        "id": "wTR0CWGmuQFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "principalDf = pd.concat([principalDf, y_train.reset_index()], axis=1)\n",
        "principalDf.drop(columns = 'index',inplace=True)\n",
        "principalDf"
      ],
      "metadata": {
        "trusted": true,
        "id": "aB37I0eGuQFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = sampled_df.drop('Severity', axis=1)\n",
        "y= sampled_df['Severity']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.scatter(principalDf['principal component 1'], principalDf['principal component 2'], c=y_train, cmap='viridis')\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Scatter Plot')\n",
        "\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Target Variable')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "T-30c2MYuQFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing PCA when n_components = 3"
      ],
      "metadata": {
        "id": "lod2wHKZuQFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = sampled_df.drop('Severity', axis=1)\n",
        "y= sampled_df['Severity']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components = 3)\n",
        "\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "principalDf = pd.DataFrame(data=X_train, columns=['principal component 1', 'principal component 2','principal component 3'])\n",
        "\n",
        "principalDf"
      ],
      "metadata": {
        "trusted": true,
        "id": "wpIPLzAAuQFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "principalDf = pd.concat([principalDf, y_train.reset_index()], axis=1)\n",
        "principalDf.drop(columns = 'index',inplace=True)\n",
        "principalDf"
      ],
      "metadata": {
        "trusted": true,
        "id": "T7ajYF52uQFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "xdata = X_train[:,0]\n",
        "ydata = X_train[:,1]\n",
        "zdata = X_train[:,2]\n",
        "\n",
        "ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='viridis')\n",
        "\n",
        "plt.title(f'3D Scatter of PCA')\n",
        "\n",
        "ticks = np.linspace(-3, 3, num=5)\n",
        "ax.set_xticks(ticks)\n",
        "ax.set_yticks(ticks)\n",
        "ax.set_zticks(ticks)\n",
        "\n",
        "ax.set_xlabel('PCA 1', rotation=150)\n",
        "ax.set_ylabel('PCA 2')\n",
        "ax.set_zlabel('PCA 3', rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "bXDmPRQWuQFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Explained Variance"
      ],
      "metadata": {
        "id": "bw85hJuvuQFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(range(1,len(pca.explained_variance_)+1),pca.explained_variance_)\n",
        "\n",
        "plt.xlabel('PCA Feature')\n",
        "plt.ylabel('Explained variance')\n",
        "plt.title('Feature Explained Variance')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "zIP5NzsIuQFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "8V80RVjMuQFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "Nn2Dm7lnuQFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voting_classifier.fit(X_train, X_test)\n"
      ],
      "metadata": {
        "id": "5Oy9pSMsuQFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = voting_classifier.predict(X_test)\n",
        "\n",
        "Eval_PCA=classification_task(voting_classifier,X_train, y_train ,X_test,y_test, y_pred,'model_PCA')\n",
        "Eval_PCA"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.718498Z",
          "iopub.status.idle": "2023-09-17T11:43:40.719278Z",
          "shell.execute_reply": "2023-09-17T11:43:40.719021Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.718995Z"
        },
        "trusted": true,
        "id": "8QQ2AiEouQFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test_resampled, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test_resampled, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test_resampled, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "eh9LpXKguQFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models Comparisons"
      ],
      "metadata": {
        "id": "ucipnzmJuQFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([Eval_KNN, Eval_voting , Eval_ada , Eval_xgb ,Eval_Rf,Eval_Dt, Eval_SVM,Eval_nb,Eval_bbc,Eval_logreg,Eval_PCA])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.720783Z",
          "iopub.status.idle": "2023-09-17T11:43:40.721615Z",
          "shell.execute_reply": "2023-09-17T11:43:40.721353Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.721326Z"
        },
        "trusted": true,
        "id": "cJFnftsAuQFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Segmentation\n",
        "**Predicting accident severity based on Description Provided**"
      ],
      "metadata": {
        "id": "7vxA5GKwuQFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Description']"
      ],
      "metadata": {
        "id": "WEL0vYIiuQFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Punctuation"
      ],
      "metadata": {
        "id": "Gd7ETkg7uQFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "print(PUNCT_TO_REMOVE)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "df[\"Description\"] = df[\"Description\"].apply(lambda text: remove_punctuation(text))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "TBwgEDmtuQFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "wjaAGJf4uQFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newdata= word_tokenize(df['Description'])\n",
        "newdata"
      ],
      "metadata": {
        "id": "ryffhUOMuQFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Alpha-numeric characters"
      ],
      "metadata": {
        "id": "gG56KI20uQFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newdata = re.sub(r'\\W', ' ', newdata)\n",
        "newdata"
      ],
      "metadata": {
        "id": "EWBVGNbEuQFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Stop Words"
      ],
      "metadata": {
        "id": "za0VNj5OuQFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "newdata= remove_stopwords(newdata)\n",
        "newdata"
      ],
      "metadata": {
        "id": "vkSPAqnyuQFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "KzIW1UTFuQFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizze(df):\n",
        "    newdata= [WordNetLemmatizer().lemmatize(t) for t in df]\n",
        "    return newdata\n",
        "\n",
        "newdata=lemmatizze(newdata)\n",
        "newdata"
      ],
      "metadata": {
        "id": "0jP2-lcguQFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata=\" \".join(newdata)\n",
        "newdata"
      ],
      "metadata": {
        "id": "O6zHrJbDuQFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning all together"
      ],
      "metadata": {
        "id": "S8isO5muuQFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def Cleaning_process(df):\n",
        "    processed_text=remove_punctuation(str(df))\n",
        "    tokenized_data=word_tokenize(processed_text.lower())\n",
        "    textwithoutnum=  re.sub(r'\\W', ' ', tokenized_data)\n",
        "    data=remove_stopwords(textwithoutnum)\n",
        "    final_data=lemmatizze(data)\n",
        "    return \" \".join(final_data)\n",
        "\n",
        "\n",
        "\n",
        "df[\"Description\"]= df[\"Description\"].apply(Cleaning_process)\n",
        "df[\"Description\"]"
      ],
      "metadata": {
        "id": "ElfrNHuOuQFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Split for Text"
      ],
      "metadata": {
        "id": "UvoPemq8uQFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df_text= df.sample(frac=0.1, random_state=42)\n",
        "X= sampled_df_text['Description']\n",
        "y= sampled_df_text['Severity']"
      ],
      "metadata": {
        "id": "D6v_7Sn-uQFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=101)"
      ],
      "metadata": {
        "id": "I8f19Vx7uQFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "oWXgl3MRuQFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "u3rYSnqsuQFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorizing"
      ],
      "metadata": {
        "id": "u2aGSwoUuQFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
        "X_test_tf_idf = tf_idf_vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "mdmgpc-juQFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tf_idf.toarray().shape"
      ],
      "metadata": {
        "id": "HrWDUxTuuQFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tf_idf.toarray().shape"
      ],
      "metadata": {
        "id": "0G0KROwYuQFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tf_idf.toarray()"
      ],
      "metadata": {
        "id": "rx2j-YrxuQFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names())"
      ],
      "metadata": {
        "id": "bOEC_00-uQFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying Models on Text\n",
        "- Since we already created objects from all models , we will only call .fit() method to assess the accuracy"
      ],
      "metadata": {
        "id": "cnCpiMU_uQFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "u2cQRhnKuQFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "Eval_rf=classification_task(rf_classifier,X_train, y_train ,X_test,y_test, y_pred,'rf')\n",
        "Eval_rf"
      ],
      "metadata": {
        "id": "qYnm27jSuQFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "IeNVOKPouQFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees"
      ],
      "metadata": {
        "id": "Jp9o8rUquQFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "Eval_dt=classification_task(dt_classifier,X_train, y_train ,X_test,y_test, y_pred,'dt')\n",
        "Eval_dt"
      ],
      "metadata": {
        "id": "Ci3u48UKuQFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "2cbz7GsluQFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "utWOyDPDuQFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn_classifier.predict(X_test)\n",
        "\n",
        "Eval_knn=classification_task(knn_classifier,X_train, y_train ,X_test,y_test, y_pred,'knn')\n",
        "Eval_knn"
      ],
      "metadata": {
        "id": "ce4HtBXiuQFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "S6w_peZVuQFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "I3NGpagOuQFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "Eval_nb=classification_task(nb_classifier,X_train, y_train ,X_test,y_test, y_pred,'nb')\n",
        "Eval_nb"
      ],
      "metadata": {
        "id": "2FWP_SiTuQFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "-FWyFy5buQFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "xUPBEsSquQFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "Eval_xgb=classification_task(xgb_classifier,X_train, y_train ,X_test,y_test, y_pred,'xgb')\n",
        "Eval_xgb"
      ],
      "metadata": {
        "id": "yS8qWuFcuQFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "yurkA0yWuQFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADABoost"
      ],
      "metadata": {
        "id": "LDSSx-JmuQFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = ada_classifier.predict(X_test)\n",
        "\n",
        "Eval_ada=classification_task(ada_classifier,X_train, y_train ,X_test,y_test, y_pred,'ada')\n",
        "Eval_ada"
      ],
      "metadata": {
        "id": "xUyuq4qquQFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "ev918eS8uQFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression (Text)"
      ],
      "metadata": {
        "id": "PJv7HuziuQFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "Eval_logreg=classification_task(logistic_regression,X_train, y_train ,X_test,y_test, y_pred,'log reg')\n",
        "Eval_logreg"
      ],
      "metadata": {
        "id": "WzYLxZ96uQFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "MecuEWpPuQFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voting (Text)\n",
        "- in this case only we need to re- instantiate voting object to add naiive bayes and remove bbc classifier . Because naiive bayes  classifieres are simple yet effective models for text classification."
      ],
      "metadata": {
        "id": "CPLd_gy9uQFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voting_classifier = VotingClassifier(estimators=[\n",
        "    ('ada', ada_classifier),\n",
        "    ('dt_classifier',dt_classifier),\n",
        "    ('rf_classifier',rf_classifier),\n",
        "    ('xgb', xgb_classifier),\n",
        "    ('logreg',logistic_regression),\n",
        "    (\"nb\",nb_classifier)\n",
        "], voting='soft')\n",
        "\n",
        "voting_classifier.fit(X_train, y_train)\n",
        "voting_classifier.fit(X_train, X_test)\n",
        "y_pred = voting_classifier.predict(X_test)\n",
        "\n",
        "Eval_voting=classification_task(voting_classifier,X_train, y_train ,X_test,y_test, y_pred,'voting')\n",
        "Eval_voting"
      ],
      "metadata": {
        "id": "Tsx1k63muQFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "Pf-s0eRRuQFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Test Split"
      ],
      "metadata": {
        "id": "CkpQNccuuQFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = sampled_df.drop([\"Distance(mi)\"] , axis = 1).values\n",
        "y = sampled_df[\"Distance(mi)\"].values"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.725192Z",
          "iopub.status.idle": "2023-09-17T11:43:40.726011Z",
          "shell.execute_reply": "2023-09-17T11:43:40.725763Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.725736Z"
        },
        "trusted": true,
        "id": "8nMbmqvfuQFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train , x_test , y_train ,y_test = train_test_split(x,y , test_size= 0.25 , random_state= 42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.727418Z",
          "iopub.status.idle": "2023-09-17T11:43:40.728207Z",
          "shell.execute_reply": "2023-09-17T11:43:40.727951Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.727925Z"
        },
        "trusted": true,
        "id": "zWZrEIc2uQFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ro_scaler = RobustScaler()\n",
        "x_train = ro_scaler.fit_transform(x_train)\n",
        "x_test = ro_scaler.fit_transform(x_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.729677Z",
          "iopub.status.idle": "2023-09-17T11:43:40.730505Z",
          "shell.execute_reply": "2023-09-17T11:43:40.730230Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.730203Z"
        },
        "trusted": true,
        "id": "ezn9q-N3uQFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "yGPXsZl1uQFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin = LinearRegression()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.736229Z",
          "iopub.status.idle": "2023-09-17T11:43:40.737051Z",
          "shell.execute_reply": "2023-09-17T11:43:40.736796Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.736769Z"
        },
        "trusted": true,
        "id": "sQ5kEj9GuQFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lin.fit(x_train,y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.738834Z",
          "iopub.status.idle": "2023-09-17T11:43:40.739660Z",
          "shell.execute_reply": "2023-09-17T11:43:40.739401Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.739356Z"
        },
        "trusted": true,
        "id": "VblRHR89uQFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lin.score(x_train,y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.741283Z",
          "iopub.status.idle": "2023-09-17T11:43:40.742410Z",
          "shell.execute_reply": "2023-09-17T11:43:40.741912Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.741885Z"
        },
        "trusted": true,
        "id": "rY9KnVOsuQFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lin.score(x_test , y_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.744011Z",
          "iopub.status.idle": "2023-09-17T11:43:40.744838Z",
          "shell.execute_reply": "2023-09-17T11:43:40.744566Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.744538Z"
        },
        "trusted": true,
        "id": "4wK8VFojuQFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lin.coef_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.746095Z",
          "iopub.status.idle": "2023-09-17T11:43:40.748138Z",
          "shell.execute_reply": "2023-09-17T11:43:40.747868Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.747838Z"
        },
        "trusted": true,
        "id": "4Y2COOt5uQFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lin.intercept_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.749608Z",
          "iopub.status.idle": "2023-09-17T11:43:40.750424Z",
          "shell.execute_reply": "2023-09-17T11:43:40.750151Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.750124Z"
        },
        "trusted": true,
        "id": "z_sct0DYuQFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DBSCAN clustering"
      ],
      "metadata": {
        "id": "XHxiLy9UuQFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that: we didn't cluster by H clustering as it is not suitable for large dataset but DBSCAN is."
      ],
      "metadata": {
        "id": "k4inZv_ruQFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN"
      ],
      "metadata": {
        "id": "_RY-UYOuuQFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Applying DBSCAN clustering on city and severity Columns**"
      ],
      "metadata": {
        "id": "InK5CmYpuQFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# working with random sample as the dataset is too large.\n",
        "sample_size = 100000  # Specify the desired sample size\n",
        "rs = df.sample(n=sample_size)"
      ],
      "metadata": {
        "id": "N5zr-LBkuQFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = rs.iloc[:, [2, 14]].values"
      ],
      "metadata": {
        "id": "q9TmEDybuQFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Specify the parameters for DBSCAN\n",
        "# eps = 0.3  # The maximum distance between two samples to be considered as neighbors\n",
        "# min_samples = 5  # The minimum number of samples in a neighborhood to be considered as a core point\n",
        "\n",
        "# # Perform DBSCAN clustering\n",
        "# dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "# labels = dbscan.fit_predict(X)\n",
        "\n",
        "# # Plot the clusters\n",
        "# plt.scatter(X, np.zeros_like(X), c=labels, cmap='viridis')\n",
        "# plt.xlabel('Column')\n",
        "# plt.ylabel('Cluster')\n",
        "# plt.title('DBSCAN Clustering')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "p7c43i5OuQFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Applying DBSCAN clustering on temperature and severity Columns**"
      ],
      "metadata": {
        "id": "sCNhKdn-uQFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = rs.iloc[:, [2, 20]].values"
      ],
      "metadata": {
        "id": "p6I3KyksuQFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Specify the parameters for DBSCAN\n",
        "# eps = 0.3  # The maximum distance between two samples to be considered as neighbors\n",
        "# min_samples = 5  # The minimum number of samples in a neighborhood to be considered as a core point\n",
        "\n",
        "# # Perform DBSCAN clustering\n",
        "# dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "# labels = dbscan.fit_predict(X)\n",
        "\n",
        "# # Plot the clusters\n",
        "# plt.scatter(X, np.zeros_like(X), c=labels, cmap='viridis')\n",
        "# plt.xlabel('Column')\n",
        "# plt.ylabel('Cluster')\n",
        "# plt.title('DBSCAN Clustering')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "EK2V7KesuQFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto ML\n",
        "\n",
        "Automated machine learning (AutoML) is the process of applying machine learning (ML) models to real-world problems using automation. More specifically, it automates the selection, composition and parameterization of ML models. Automating the machine learning process makes it more user-friendly and often provides faster, more accurate outputs than hand-coded algorithms.\n",
        "\n",
        "AutoML software platforms make machine learning more user-friendly and give organizations without a specialized data scientist or ML expert access to machine learning. These platforms can be acquired from a third-party vendor, accessed through open source repositories such as GitHub or built in house.\n",
        "How does the AutoML process work?\n",
        "\n",
        "AutoML is typically a platform or open source library that simplifies each step in the machine learning process, from handling a raw data set to deploying a practical ML model. In traditional machine learning, models are developed by hand, and each step in the process must be handled separately.\n",
        "A diagram depicting the AutoML process.\n",
        "\n",
        "AutoML automatically locates and uses the optimal type of machine learning algorithm for a given task. It does this with two concepts:\n",
        "\n",
        "    Neural architecture search. This automates the design of neural networks. It helps AutoML models discover new architectures for problems that require them.\n",
        "    Transfer learning. Pre-trained models apply what they've learned to new data sets. Transfer learning helps AutoML apply existing architectures to new problems that require it.\n"
      ],
      "metadata": {
        "id": "3vfvlBmUuQFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![automl](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "dy8Iyf4luQFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## H2O"
      ],
      "metadata": {
        "id": "pz4nYJJ5uQFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h2o.init()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.753943Z",
          "iopub.status.idle": "2023-09-17T11:43:40.754722Z",
          "shell.execute_reply": "2023-09-17T11:43:40.754466Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.754439Z"
        },
        "trusted": true,
        "id": "aJTHzNt5uQFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = sampled_df.shape[0]-200\n",
        "df_train = sampled_df.iloc[:sample_size].copy()\n",
        "df_test = sampled_df.iloc[sample_size:].copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.756075Z",
          "iopub.status.idle": "2023-09-17T11:43:40.756889Z",
          "shell.execute_reply": "2023-09-17T11:43:40.756629Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.756602Z"
        },
        "trusted": true,
        "id": "zKLiubPauQFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h2o_frame= h2o.H2OFrame(df_train)\n",
        "x= h2o_frame.columns\n",
        "y= 'Severity'\n",
        "x.remove(y)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.758515Z",
          "iopub.status.idle": "2023-09-17T11:43:40.759324Z",
          "shell.execute_reply": "2023-09-17T11:43:40.759063Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.759037Z"
        },
        "trusted": true,
        "id": "L02c2AbFuQFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h2o_automl= H2OAutoML(max_runtime_secs= 300, seed = 666)\n",
        "h2o_automl.train(x=x, y=y, training_frame= h2o_frame)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.760755Z",
          "iopub.status.idle": "2023-09-17T11:43:40.761551Z",
          "shell.execute_reply": "2023-09-17T11:43:40.761284Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.761257Z"
        },
        "trusted": true,
        "id": "XsQMbQAnuQFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h2o_frame_test = h2o.H2OFrame(df_test)\n",
        "y_pred = h2o_automl.predict(h2o_frame_test)\n",
        "y_actual = h2o.H2OFrame(df_test[['Severity']])\n",
        "\n",
        "y_pred_pd = y_pred.as_data_frame()\n",
        "y_actual_pd = y_actual.as_data_frame()\n",
        "\n",
        "y_pred_labels = y_pred_pd.idxmax(axis=1)\n",
        "y_actual_labels = y_actual_pd['Severity'].values\n",
        "\n",
        "accuracy_score(y_actual_labels, y_pred_labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.762976Z",
          "iopub.status.idle": "2023-09-17T11:43:40.763767Z",
          "shell.execute_reply": "2023-09-17T11:43:40.763513Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.763486Z"
        },
        "trusted": true,
        "id": "1DxHEtfouQFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tpot\n",
        "![Tpot](https://machinelearningmastery.com/wp-content/uploads/2020/03/Overview-of-the-TPOT-Pipeline-Search.png)"
      ],
      "metadata": {
        "id": "P7Tqc7t3uQFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tpot_automl = TPOTClassifier(generations =10 , population_size= 10, verbosity = 2,\n",
        "                            max_time_mins= 5, random_state= 666)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.767241Z",
          "iopub.status.idle": "2023-09-17T11:43:40.768041Z",
          "shell.execute_reply": "2023-09-17T11:43:40.767798Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.767772Z"
        },
        "trusted": true,
        "id": "uB4Ib1d1uQFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = df_train.drop(columns= 'Severity')\n",
        "y_train = df_train['Severity']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.769418Z",
          "iopub.status.idle": "2023-09-17T11:43:40.770191Z",
          "shell.execute_reply": "2023-09-17T11:43:40.769949Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.769922Z"
        },
        "trusted": true,
        "id": "1zDK9TDGuQFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpot_automl.fit(x_train,y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.772868Z",
          "iopub.status.idle": "2023-09-17T11:43:40.773803Z",
          "shell.execute_reply": "2023-09-17T11:43:40.773362Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.773310Z"
        },
        "trusted": true,
        "id": "GeS4at4YuQFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = df_test.drop(columns ='Severity')\n",
        "y_test= df_test['Severity']\n",
        "\n",
        "tpot_automl.score(x_test,y_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.775089Z",
          "iopub.status.idle": "2023-09-17T11:43:40.775700Z",
          "shell.execute_reply": "2023-09-17T11:43:40.775399Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.775354Z"
        },
        "trusted": true,
        "id": "lLqtmcLduQFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred= tpot_automl.predict(x_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.776779Z",
          "iopub.status.idle": "2023-09-17T11:43:40.777350Z",
          "shell.execute_reply": "2023-09-17T11:43:40.777100Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.777074Z"
        },
        "trusted": true,
        "id": "9VtmBe8juQFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyCaret\n",
        "![pycaret](https://miro.medium.com/v2/resize:fit:1024/1*Cku5-rqmqSIuhUyFkIAdIA.png)"
      ],
      "metadata": {
        "id": "L9sh7IkfuQFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pycaret_automl= setup(data = df_train , target = 'Severity',session_id = 666)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.779960Z",
          "iopub.status.idle": "2023-09-17T11:43:40.780504Z",
          "shell.execute_reply": "2023-09-17T11:43:40.780246Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.780221Z"
        },
        "trusted": true,
        "id": "2mF0hTUZuQFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pycaret_models = compare_models(budget_time=5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.783172Z",
          "iopub.status.idle": "2023-09-17T11:43:40.783744Z",
          "shell.execute_reply": "2023-09-17T11:43:40.783498Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.783472Z"
        },
        "trusted": true,
        "id": "OWXV6v1nuQFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict_model(pycaret_models,data = df_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.786886Z",
          "iopub.status.idle": "2023-09-17T11:43:40.788834Z",
          "shell.execute_reply": "2023-09-17T11:43:40.788634Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.788610Z"
        },
        "trusted": true,
        "id": "m0aVnEQkuQFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = sampled_df.drop([\"Severity\"] , axis = 1)\n",
        "y = sampled_df[\"Severity\"].values\n",
        "x_train , x_test , y_train ,y_test = train_test_split(x,y , test_size= 0.25 , random_state= 42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.789746Z",
          "iopub.status.idle": "2023-09-17T11:43:40.790688Z",
          "shell.execute_reply": "2023-09-17T11:43:40.790449Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.790421Z"
        },
        "trusted": true,
        "id": "cM7mCvWBuQFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evalml\n",
        "![evalml](https://editor.analyticsvidhya.com/uploads/64432Screenshot%20(645).png)"
      ],
      "metadata": {
        "id": "wulbJe0suQFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test, y_train, y_test= evalml.preprocessing.split_data(x,y,problem_type ='MULTICLASS')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.793947Z",
          "iopub.status.idle": "2023-09-17T11:43:40.794451Z",
          "shell.execute_reply": "2023-09-17T11:43:40.794212Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.794193Z"
        },
        "trusted": true,
        "id": "uIW6fGj8uQFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automl= AutoMLSearch(X_train= x_train , y_train= y_train, problem_type='multiclass')\n",
        "automl.search()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.796938Z",
          "iopub.status.idle": "2023-09-17T11:43:40.797501Z",
          "shell.execute_reply": "2023-09-17T11:43:40.797244Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.797219Z"
        },
        "trusted": true,
        "id": "rQGxKb3ZuQFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automl.rankings"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.798492Z",
          "iopub.status.idle": "2023-09-17T11:43:40.799019Z",
          "shell.execute_reply": "2023-09-17T11:43:40.798780Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.798755Z"
        },
        "trusted": true,
        "id": "toinxbb9uQFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_pipeline =automl.best_pipeline\n",
        "best_pipeline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.801228Z",
          "iopub.status.idle": "2023-09-17T11:43:40.801677Z",
          "shell.execute_reply": "2023-09-17T11:43:40.801483Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.801463Z"
        },
        "trusted": true,
        "id": "TjLSXBGyuQFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automl.describe_pipeline(automl.rankings.iloc[0]['id'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.802886Z",
          "iopub.status.idle": "2023-09-17T11:43:40.804829Z",
          "shell.execute_reply": "2023-09-17T11:43:40.804559Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.804533Z"
        },
        "trusted": true,
        "id": "7U8_RtvPuQFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_pipeline.score(x_test,y_test, objectives = ['F1 Weighted','F1 Macro','F1 Micro','Accuracy Multiclass','Precision Weighted','Precision Macro','Precision Micro'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.806246Z",
          "iopub.status.idle": "2023-09-17T11:43:40.807042Z",
          "shell.execute_reply": "2023-09-17T11:43:40.806790Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.806763Z"
        },
        "trusted": true,
        "id": "fJB_IVmfuQF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for objective in get_optimization_objectives(ProblemTypes.MULTICLASS):\n",
        "    print(objective.name)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.808454Z",
          "iopub.status.idle": "2023-09-17T11:43:40.809214Z",
          "shell.execute_reply": "2023-09-17T11:43:40.808976Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.808948Z"
        },
        "trusted": true,
        "id": "7sjDoY_fuQF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automl_customized= AutoMLSearch(X_train= x_train, y_train = y_train,\n",
        "                               problem_type='multiclass',\n",
        "                               objective= 'F1 Weighted',\n",
        "                               additional_objectives=['F1 Macro','F1 Micro','Accuracy Multiclass','Precision Weighted','Precision Macro','Precision Micro'],\n",
        "                               max_batches=1,\n",
        "                               optimize_thresholds= True)\n",
        "automl_customized.search()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.810566Z",
          "iopub.status.idle": "2023-09-17T11:43:40.811268Z",
          "shell.execute_reply": "2023-09-17T11:43:40.811050Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.811026Z"
        },
        "trusted": true,
        "id": "VKo07B-KuQF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automl_customized.rankings"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.812555Z",
          "iopub.status.idle": "2023-09-17T11:43:40.813306Z",
          "shell.execute_reply": "2023-09-17T11:43:40.813066Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.813041Z"
        },
        "trusted": true,
        "id": "M6Bo-nG2uQF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automl_customized.describe_pipeline(automl_customized.rankings.iloc[0]['id'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-17T11:43:40.814728Z",
          "iopub.status.idle": "2023-09-17T11:43:40.815523Z",
          "shell.execute_reply": "2023-09-17T11:43:40.815247Z",
          "shell.execute_reply.started": "2023-09-17T11:43:40.815219Z"
        },
        "trusted": true,
        "id": "pQlDc9GfuQF0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}